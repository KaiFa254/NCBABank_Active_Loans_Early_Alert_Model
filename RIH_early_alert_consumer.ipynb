{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "#from fitter import Fitter, get_common_distributions, get_distributions\n",
    "#import sweetviz as sv\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import jaydebeapi\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "#more\n",
    "%matplotlib inline\n",
    "import optbinning\n",
    "import tqdm\n",
    "\n",
    "#from optbinning import Scorecard, BinningProcess\n",
    "#from optbinning import OptimalBinning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from optbinning.scorecard import plot_auc_roc, plot_cap, plot_ks\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display options settings\n",
    "import locale\n",
    "\n",
    "# Set the locale to the user's default setting\n",
    "locale.setlocale(locale.LC_ALL, '')\n",
    "\n",
    "# Define a function to format floats/ints with thousands separators\n",
    "def format_with_commas(x):\n",
    "    if isinstance(x, float):\n",
    "        return '{:,.4f}'.format(x)\n",
    "    elif isinstance(x, int):\n",
    "        return '{:,}'.format(x)\n",
    "    return x\n",
    "# Set the display format for floating-point numbers with thousands separators\n",
    "pd.options.display.float_format = format_with_commas\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\erastus.kaiba\\db_credentials\")\n",
    "\n",
    "from db_utils import run_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNAPSHOT_DATE</th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>LOAN_ID</th>\n",
       "      <th>ACCOUNT_NUMBER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>CATEGORY_DESC</th>\n",
       "      <th>CURRENCY</th>\n",
       "      <th>EMI</th>\n",
       "      <th>INTEREST_RATE</th>\n",
       "      <th>EXPOSURE_ACTUAL</th>\n",
       "      <th>EXPOSURE_KES</th>\n",
       "      <th>DPD</th>\n",
       "      <th>MOB</th>\n",
       "      <th>TARGET_DEFAULT_90D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>710286</td>\n",
       "      <td>AA20249001MB</td>\n",
       "      <td>7102860063</td>\n",
       "      <td>3215</td>\n",
       "      <td>Mortgage Loan</td>\n",
       "      <td>KES</td>\n",
       "      <td>15561.820000</td>\n",
       "      <td>13.3500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-31 00:00:00</td>\n",
       "      <td>710286</td>\n",
       "      <td>AA20249001MB</td>\n",
       "      <td>7102860063</td>\n",
       "      <td>3215</td>\n",
       "      <td>Mortgage Loan</td>\n",
       "      <td>KES</td>\n",
       "      <td>15561.860000</td>\n",
       "      <td>13.3500</td>\n",
       "      <td>1054047</td>\n",
       "      <td>1,054,047.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-31 00:00:00</td>\n",
       "      <td>710286</td>\n",
       "      <td>AA20249001MB</td>\n",
       "      <td>7102860063</td>\n",
       "      <td>3215</td>\n",
       "      <td>Mortgage Loan</td>\n",
       "      <td>KES</td>\n",
       "      <td>15561.860000</td>\n",
       "      <td>13.3500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-30 00:00:00</td>\n",
       "      <td>710286</td>\n",
       "      <td>AA20249001MB</td>\n",
       "      <td>7102860063</td>\n",
       "      <td>3215</td>\n",
       "      <td>Mortgage Loan</td>\n",
       "      <td>KES</td>\n",
       "      <td>15561.860000</td>\n",
       "      <td>13.3500</td>\n",
       "      <td>1046097</td>\n",
       "      <td>1,046,097.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-10-31 00:00:00</td>\n",
       "      <td>710286</td>\n",
       "      <td>AA20249001MB</td>\n",
       "      <td>7102860063</td>\n",
       "      <td>3215</td>\n",
       "      <td>Mortgage Loan</td>\n",
       "      <td>KES</td>\n",
       "      <td>15561.890000</td>\n",
       "      <td>13.3500</td>\n",
       "      <td>1042999</td>\n",
       "      <td>1,042,999.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SNAPSHOT_DATE CUSTOMER_ID       LOAN_ID ACCOUNT_NUMBER CATEGORY  \\\n",
       "0  2025-06-30 00:00:00      710286  AA20249001MB     7102860063     3215   \n",
       "1  2025-07-31 00:00:00      710286  AA20249001MB     7102860063     3215   \n",
       "2  2025-08-31 00:00:00      710286  AA20249001MB     7102860063     3215   \n",
       "3  2025-09-30 00:00:00      710286  AA20249001MB     7102860063     3215   \n",
       "4  2025-10-31 00:00:00      710286  AA20249001MB     7102860063     3215   \n",
       "\n",
       "   CATEGORY_DESC CURRENCY           EMI  INTEREST_RATE  EXPOSURE_ACTUAL  \\\n",
       "0  Mortgage Loan      KES  15561.820000        13.3500                0   \n",
       "1  Mortgage Loan      KES  15561.860000        13.3500          1054047   \n",
       "2  Mortgage Loan      KES  15561.860000        13.3500                0   \n",
       "3  Mortgage Loan      KES  15561.860000        13.3500          1046097   \n",
       "4  Mortgage Loan      KES  15561.890000        13.3500          1042999   \n",
       "\n",
       "    EXPOSURE_KES  DPD  MOB  TARGET_DEFAULT_90D  \n",
       "0         0.0000    0   57                   0  \n",
       "1 1,054,047.0000    0   58                   0  \n",
       "2         0.0000    0   59                   0  \n",
       "3 1,046,097.0000    0   60                   0  \n",
       "4 1,042,999.0000    0   61                   0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "base_df = run_query(\"\"\"WITH SnapshotDates AS (\n",
    "    SELECT DISTINCT EXTRACTION_DATE AS SNAPSHOT_DATE \n",
    "    FROM dbcba.KE_Accounts_Master\n",
    "    WHERE EXTRACTION_DATE IN (\n",
    "        '2025-06-30','2025-07-31','2025-08-31',\n",
    "        '2025-09-30','2025-10-31','2025-11-30'\n",
    "    )\n",
    "),\n",
    "OutcomeWindow AS (\n",
    "    SELECT\n",
    "        s.SNAPSHOT_DATE,\n",
    "        a.ARRANGEMENT_ID AS LOAN_ID,\n",
    "        MAX(a.Days_In_Arrears) AS MAX_DPD_90D\n",
    "    FROM SnapshotDates s\n",
    "    JOIN STGKE.STG_AA_ARREARS a\n",
    "        ON a.EXTRACTION_DATE > s.SNAPSHOT_DATE\n",
    "        AND a.EXTRACTION_DATE <= DATEADD(DAY,90,s.SNAPSHOT_DATE)\n",
    "    GROUP BY s.SNAPSHOT_DATE, a.ARRANGEMENT_ID\n",
    ")\n",
    "SELECT \n",
    "    s.SNAPSHOT_DATE,\n",
    "    cm.CUSTOMER_NUMBER AS CUSTOMER_ID,\n",
    "    am.ARRANGEMENT_ID AS LOAN_ID,\n",
    "    am.ACCOUNT_NUMBER,\n",
    "    am.CATEGORY,\n",
    "    am.CATEGORY_DESC,\n",
    "    am.CURRENCY,\n",
    "    am.REPAYMENT_AMT AS EMI,\n",
    "    am.INTEREST_RATE,\n",
    "    CONVERT(DECIMAL(18,0), COALESCE(ex.Deal_balance,0)) AS EXPOSURE_ACTUAL,\n",
    "    CASE \n",
    "        WHEN am.CURRENCY='KES'\n",
    "        THEN CONVERT(DECIMAL(18,0), COALESCE(ex.Deal_balance,0))\n",
    "        ELSE CONVERT(DECIMAL(18,0), COALESCE(ex.Deal_balance,0)) * am.EXCHANGE_RATE\n",
    "    END AS EXPOSURE_KES,\n",
    "    COALESCE(pdo.Days_In_Arrears,0) AS DPD,\n",
    "    DATEDIFF(MONTH, am.VALUE_DATE, s.SNAPSHOT_DATE) AS MOB,\n",
    "    CASE WHEN o.MAX_DPD_90D >= 20 THEN 1 ELSE 0 END AS TARGET_DEFAULT_90D\n",
    "FROM SnapshotDates s\n",
    "JOIN dbcba.KE_Accounts_Master am\n",
    "    ON am.EXTRACTION_DATE = s.SNAPSHOT_DATE\n",
    "JOIN dbcba.KE_Customer_Master cm\n",
    "    ON cm.CUSTOMER_NUMBER = am.CUSTOMER\n",
    "    AND cm.EXTRACTION_DATE = s.SNAPSHOT_DATE\n",
    "LEFT JOIN stgke.STG_ECB_TOTAL_EXPOSURE ex\n",
    "    ON ex.RECID = am.ACCOUNT_NUMBER\n",
    "    AND ex.LOAD_DATE = s.SNAPSHOT_DATE\n",
    "LEFT JOIN STGKE.STG_AA_ARREARS pdo\n",
    "    ON pdo.ARRANGEMENT_ID = am.ARRANGEMENT_ID\n",
    "    AND pdo.EXTRACTION_DATE = s.SNAPSHOT_DATE\n",
    "LEFT JOIN OutcomeWindow o\n",
    "    ON o.LOAN_ID = am.ARRANGEMENT_ID\n",
    "    AND o.SNAPSHOT_DATE = s.SNAPSHOT_DATE\n",
    "WHERE am.PRODUCT_LINE='LENDING'\n",
    "AND am.ARR_STATUS='CURRENT'\n",
    "AND cm.BUSINESS_SEGMENT IN ('270','250')\n",
    "AND COALESCE(am.ONLINE_ACTUAL_BAL,0) < 0\n",
    "AND COALESCE(pdo.Days_In_Arrears,0)=0\n",
    "AND cm.CUSTOMER_NUMBER IS NOT NULL;\n",
    "\n",
    "\"\"\")\n",
    "base_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNAPSHOT_DATE</th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>Q1_DTO</th>\n",
       "      <th>Q2_DTO</th>\n",
       "      <th>Q3_DTO</th>\n",
       "      <th>Q4_DTO</th>\n",
       "      <th>M1M2_Change</th>\n",
       "      <th>M2M3_Change</th>\n",
       "      <th>M3M4_Change</th>\n",
       "      <th>Q1Q2_Change</th>\n",
       "      <th>Q2Q3_Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000003</td>\n",
       "      <td>123,999.1500</td>\n",
       "      <td>186,712.1000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>-0.1300</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000009</td>\n",
       "      <td>133,923.0500</td>\n",
       "      <td>1,186,038.7200</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.6100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>-0.8900</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000011</td>\n",
       "      <td>243,785.2500</td>\n",
       "      <td>51,902.3500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>-0.3500</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>3.7000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000012</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>43,706.9500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000014</td>\n",
       "      <td>75,923.4500</td>\n",
       "      <td>74,986.2000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>1.3200</td>\n",
       "      <td>-0.7700</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SNAPSHOT_DATE CUSTOMER_ID       Q1_DTO         Q2_DTO  Q3_DTO  \\\n",
       "0  2025-06-30 00:00:00     1000003 123,999.1500   186,712.1000  0.0000   \n",
       "1  2025-06-30 00:00:00     1000009 133,923.0500 1,186,038.7200  0.0000   \n",
       "2  2025-06-30 00:00:00     1000011 243,785.2500    51,902.3500  0.0000   \n",
       "3  2025-06-30 00:00:00     1000012       0.0000    43,706.9500  0.0000   \n",
       "4  2025-06-30 00:00:00     1000014  75,923.4500    74,986.2000  0.0000   \n",
       "\n",
       "   Q4_DTO  M1M2_Change  M2M3_Change  M3M4_Change  Q1Q2_Change  Q2Q3_Change  \n",
       "0  0.0000      -1.0000       0.0100      -0.1300      -0.3400          NaN  \n",
       "1  0.0000      -0.6100          NaN      -1.0000      -0.8900          NaN  \n",
       "2  0.0000       0.2100      -0.3500       0.9800       3.7000          NaN  \n",
       "3  0.0000          NaN          NaN          NaN      -1.0000          NaN  \n",
       "4  0.0000       0.2800       1.3200      -0.7700       0.0100          NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dto_df = run_query(\"\"\" \n",
    "/* ==========================================\n",
    "   SNAPSHOT + TRENDED DTO (SIMPLE VERSION)\n",
    "   ========================================== */\n",
    "\n",
    "WITH SnapshotDates AS (\n",
    "    SELECT DISTINCT EXTRACTION_DATE AS SNAPSHOT_DATE\n",
    "    FROM dbcba.KE_Accounts_Master\n",
    "    WHERE EXTRACTION_DATE IN (\n",
    "        '2025-06-30','2025-07-31','2025-08-31',\n",
    "        '2025-09-30','2025-10-31','2025-11-30'\n",
    "    )\n",
    "),\n",
    "FilteredCustomers AS (\n",
    "    SELECT am.CUSTOMER\n",
    "    FROM dbcba.KE_Accounts_List am\n",
    "    LEFT JOIN dbcba.KE_Customer_Master cm\n",
    "        ON cm.CUSTOMER_NUMBER = am.CUSTOMER\n",
    "    WHERE am.PRODUCT_LINE='LENDING'\n",
    "      AND am.ARR_STATUS='CURRENT'\n",
    "      AND cm.BUSINESS_SEGMENT IN ('270','250')\n",
    "      AND COALESCE(am.ONLINE_ACTUAL_BAL,0) < 0\n",
    "      AND am.CUSTOMER IS NOT NULL\n",
    "),\n",
    "BaseData AS (\n",
    "    SELECT\n",
    "        s.SNAPSHOT_DATE,\n",
    "        t.CUSTOMER_ID,\n",
    "        t.BOOKING_DATE,\n",
    "        CAST(t.AMOUNT_LCY * -1 AS DECIMAL(18,2)) AS AMOUNT_LCY_1\n",
    "    FROM SnapshotDates s\n",
    "    JOIN dbcba.KE_ACCOUNT_TRANSACTIONS t\n",
    "        ON t.BOOKING_DATE <= s.SNAPSHOT_DATE\n",
    "       AND t.BOOKING_DATE > DATEADD(MONTH,-12,s.SNAPSHOT_DATE)\n",
    "    WHERE t.AMOUNT_LCY < 0\n",
    "      AND t.REVERSAL_MARKER <> 'R'\n",
    "        AND t.CUSTOMER_ID IS NOT NULL\n",
    "      AND (t.DEBIT_CUSTOMER <> t.CREDIT_CUSTOMER OR t.CREDIT_CUSTOMER IS NULL)\n",
    "      AND t.TRANSACTION_CODE NOT IN\n",
    "          ('433','234','938','939','940','941','991','992')\n",
    "        AND t.CUSTOMER_ID IN (SELECT CUSTOMER FROM FilteredCustomers)\n",
    "),\n",
    "\n",
    "/* ==========================================\n",
    "   Month-level Aggregation\n",
    "   ========================================== */\n",
    "\n",
    "Monthly AS (\n",
    "\n",
    "SELECT\n",
    "    SNAPSHOT_DATE,\n",
    "    CUSTOMER_ID,\n",
    "\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 0 THEN AMOUNT_LCY_1 ELSE 0 END) AS M1,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 1 THEN AMOUNT_LCY_1 ELSE 0 END) AS M2,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 2 THEN AMOUNT_LCY_1 ELSE 0 END) AS M3,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 3 THEN AMOUNT_LCY_1 ELSE 0 END) AS M4,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 4 THEN AMOUNT_LCY_1 ELSE 0 END) AS M5,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 5 THEN AMOUNT_LCY_1 ELSE 0 END) AS M6,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 6 THEN AMOUNT_LCY_1 ELSE 0 END) AS M7,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 7 THEN AMOUNT_LCY_1 ELSE 0 END) AS M8,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 8 THEN AMOUNT_LCY_1 ELSE 0 END) AS M9,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 9 THEN AMOUNT_LCY_1 ELSE 0 END) AS M10,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 10 THEN AMOUNT_LCY_1 ELSE 0 END) AS M11,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 11 THEN AMOUNT_LCY_1 ELSE 0 END) AS M12,\n",
    "\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 0 THEN 1 END) AS C1,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 1 THEN 1 END) AS C2,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 2 THEN 1 END) AS C3,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 3 THEN 1 END) AS C4,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 4 THEN 1 END) AS C5,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 5 THEN 1 END) AS C6,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 6 THEN 1 END) AS C7,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 7 THEN 1 END) AS C8,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 8 THEN 1 END) AS C9,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 9 THEN 1 END) AS C10,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 10 THEN 1 END) AS C11,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE) = 11 THEN 1 END) AS C12\n",
    "\n",
    "FROM BaseData\n",
    "GROUP BY SNAPSHOT_DATE, CUSTOMER_ID\n",
    ")\n",
    "\n",
    "/* ==========================================\n",
    "   Final Output With Trends\n",
    "   ========================================== */\n",
    "\n",
    "SELECT\n",
    "    SNAPSHOT_DATE,\n",
    "    CUSTOMER_ID,\n",
    "\n",
    "    /* Quarter Totals */\n",
    "    (M1+M2+M3)  AS Q1_DTO,\n",
    "    (M4+M5+M6)  AS Q2_DTO,\n",
    "    (M7+M8+M9)  AS Q3_DTO,\n",
    "    (M10+M11+M12) AS Q4_DTO,\n",
    "\n",
    "    /* Month-on-Month */\n",
    "    CASE WHEN M2=0 THEN NULL ELSE ROUND((M1-M2)/NULLIF(M2,0),2) END AS M1M2_Change,\n",
    "    CASE WHEN M3=0 THEN NULL ELSE ROUND((M2-M3)/NULLIF(M3,0),2) END AS M2M3_Change,\n",
    "    CASE WHEN M4=0 THEN NULL ELSE ROUND((M3-M4)/NULLIF(M4,0),2) END AS M3M4_Change,\n",
    "\n",
    "    /* Quarter-on-Quarter */\n",
    "    CASE WHEN (M4+M5+M6)=0 THEN NULL\n",
    "         ELSE ROUND(((M1+M2+M3)-(M4+M5+M6))\n",
    "              / NULLIF((M4+M5+M6),0),2)\n",
    "    END AS Q1Q2_Change,\n",
    "\n",
    "    CASE WHEN (M7+M8+M9)=0 THEN NULL\n",
    "         ELSE ROUND(((M4+M5+M6)-(M7+M8+M9))\n",
    "              / NULLIF((M7+M8+M9),0),2)\n",
    "    END AS Q2Q3_Change\n",
    "\n",
    "FROM Monthly\n",
    "ORDER BY SNAPSHOT_DATE, CUSTOMER_ID;\n",
    "\n",
    "\"\"\")\n",
    "dto_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNAPSHOT_DATE</th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>Q1_CTO</th>\n",
       "      <th>Q2_CTO</th>\n",
       "      <th>Q3_CTO</th>\n",
       "      <th>Q4_CTO</th>\n",
       "      <th>Q1_CTO_Count</th>\n",
       "      <th>Q2_CTO_Count</th>\n",
       "      <th>Q3_CTO_Count</th>\n",
       "      <th>Q4_CTO_Count</th>\n",
       "      <th>M1M2_CTO_Amt_Change</th>\n",
       "      <th>M2M3_CTO_Amt_Change</th>\n",
       "      <th>M3M4_CTO_Amt_Change</th>\n",
       "      <th>M4M5_CTO_Amt_Change</th>\n",
       "      <th>M5M6_CTO_Amt_Change</th>\n",
       "      <th>M6M7_CTO_Amt_Change</th>\n",
       "      <th>M7M8_CTO_Amt_Change</th>\n",
       "      <th>M8M9_CTO_Amt_Change</th>\n",
       "      <th>M9M10_CTO_Amt_Change</th>\n",
       "      <th>M10M11_CTO_Amt_Change</th>\n",
       "      <th>M11M12_CTO_Amt_Change</th>\n",
       "      <th>Q1Q2_CTO_Amt_Change</th>\n",
       "      <th>Q2Q3_CTO_Amt_Change</th>\n",
       "      <th>Q3Q4_CTO_Amt_Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000003</td>\n",
       "      <td>124,000.0000</td>\n",
       "      <td>185,763.3800</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>-0.1100</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.3300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000009</td>\n",
       "      <td>112,500.0000</td>\n",
       "      <td>77,000.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>17.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000011</td>\n",
       "      <td>240,000.0000</td>\n",
       "      <td>82,000.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.9300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000012</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>44,000.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>4.2900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000014</td>\n",
       "      <td>78,981.0000</td>\n",
       "      <td>63,579.7400</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>1.5400</td>\n",
       "      <td>-0.7400</td>\n",
       "      <td>3.2400</td>\n",
       "      <td>-0.1100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SNAPSHOT_DATE CUSTOMER_ID       Q1_CTO       Q2_CTO  Q3_CTO  Q4_CTO  \\\n",
       "0  2025-06-30 00:00:00     1000003 124,000.0000 185,763.3800  0.0000  0.0000   \n",
       "1  2025-06-30 00:00:00     1000009 112,500.0000  77,000.0000  0.0000  0.0000   \n",
       "2  2025-06-30 00:00:00     1000011 240,000.0000  82,000.0000  0.0000  0.0000   \n",
       "3  2025-06-30 00:00:00     1000012       0.0000  44,000.0000  0.0000  0.0000   \n",
       "4  2025-06-30 00:00:00     1000014  78,981.0000  63,579.7400  0.0000  0.0000   \n",
       "\n",
       "   Q1_CTO_Count  Q2_CTO_Count  Q3_CTO_Count  Q4_CTO_Count  \\\n",
       "0             2             4             0             0   \n",
       "1             3             4             0             0   \n",
       "2             3             2             0             0   \n",
       "3             0            17             0             0   \n",
       "4             6             5             0             0   \n",
       "\n",
       "   M1M2_CTO_Amt_Change  M2M3_CTO_Amt_Change  M3M4_CTO_Amt_Change  \\\n",
       "0              -1.0000               0.0100              -0.1100   \n",
       "1               0.0000               0.0000               0.0000   \n",
       "2               0.0000               0.0000               0.0000   \n",
       "3                  NaN                  NaN                  NaN   \n",
       "4               0.4300               1.5400              -0.7400   \n",
       "\n",
       "   M4M5_CTO_Amt_Change  M5M6_CTO_Amt_Change  M6M7_CTO_Amt_Change  \\\n",
       "0               0.0600               0.2700                  NaN   \n",
       "1               0.0000              17.7500                  NaN   \n",
       "2                  NaN              -1.0000                  NaN   \n",
       "3              -1.0000               4.2900                  NaN   \n",
       "4               3.2400              -0.1100                  NaN   \n",
       "\n",
       "   M7M8_CTO_Amt_Change  M8M9_CTO_Amt_Change  M9M10_CTO_Amt_Change  \\\n",
       "0                  NaN                  NaN                   NaN   \n",
       "1                  NaN                  NaN                   NaN   \n",
       "2                  NaN                  NaN                   NaN   \n",
       "3                  NaN                  NaN                   NaN   \n",
       "4                  NaN                  NaN                   NaN   \n",
       "\n",
       "   M10M11_CTO_Amt_Change  M11M12_CTO_Amt_Change  Q1Q2_CTO_Amt_Change  \\\n",
       "0                    NaN                    NaN              -0.3300   \n",
       "1                    NaN                    NaN               0.4600   \n",
       "2                    NaN                    NaN               1.9300   \n",
       "3                    NaN                    NaN              -1.0000   \n",
       "4                    NaN                    NaN               0.2400   \n",
       "\n",
       "   Q2Q3_CTO_Amt_Change  Q3Q4_CTO_Amt_Change  \n",
       "0                  NaN                  NaN  \n",
       "1                  NaN                  NaN  \n",
       "2                  NaN                  NaN  \n",
       "3                  NaN                  NaN  \n",
       "4                  NaN                  NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cto_df = run_query(\"\"\" \n",
    "    /* ==========================================\n",
    "   SNAPSHOT + TRENDED CTO (MODEL ALIGNED)\n",
    "   ========================================== */\n",
    "\n",
    "WITH SnapshotDates AS (\n",
    "    SELECT DISTINCT EXTRACTION_DATE AS SNAPSHOT_DATE\n",
    "    FROM dbcba.KE_Accounts_Master\n",
    "    WHERE EXTRACTION_DATE IN (\n",
    "        '2025-06-30','2025-07-31','2025-08-31',\n",
    "        '2025-09-30','2025-10-31','2025-11-30'\n",
    "    )\n",
    "),\n",
    "\n",
    "FilteredCustomers AS (\n",
    "    SELECT am.CUSTOMER\n",
    "    FROM dbcba.KE_Accounts_List am\n",
    "    LEFT JOIN dbcba.KE_Customer_Master cm\n",
    "        ON cm.CUSTOMER_NUMBER = am.CUSTOMER\n",
    "    WHERE am.PRODUCT_LINE='LENDING'\n",
    "      AND am.ARR_STATUS='CURRENT'\n",
    "      AND cm.BUSINESS_SEGMENT IN ('270','250')\n",
    "      AND COALESCE(am.ONLINE_ACTUAL_BAL,0) < 0\n",
    "      AND am.CUSTOMER IS NOT NULL\n",
    "),\n",
    "                                    \n",
    "/* ==========================================\n",
    "   Base Credit Transactions (12M Rolling)\n",
    "   ========================================== */\n",
    "\n",
    "BaseCredits AS (\n",
    "    SELECT\n",
    "        s.SNAPSHOT_DATE,\n",
    "        t.CUSTOMER_ID,\n",
    "        t.BOOKING_DATE,\n",
    "        CAST(t.AMOUNT_LCY AS DECIMAL(18,2)) AS AMOUNT_LCY\n",
    "    FROM SnapshotDates s\n",
    "    JOIN dbcba.KE_ACCOUNT_TRANSACTIONS t\n",
    "        ON t.BOOKING_DATE <= s.SNAPSHOT_DATE\n",
    "       AND t.BOOKING_DATE > DATEADD(MONTH,-12,s.SNAPSHOT_DATE)\n",
    "\n",
    "    WHERE t.AMOUNT_LCY > 0\n",
    "      AND t.REVERSAL_MARKER <> 'R'\n",
    "    AND t.CUSTOMER_ID IS NOT NULL\n",
    "      AND (t.DEBIT_CUSTOMER <> t.CREDIT_CUSTOMER OR t.DEBIT_CUSTOMER IS NULL)\n",
    "      AND t.SYSTEM_ID NOT IN ('AA')\n",
    "      AND t.TRANSACTION_CODE NOT IN (\n",
    "            '1001','944','945','1085','766','949','991','941',\n",
    "            '433','1006','85','234','859','940','992',\n",
    "            '947','939','946','948','938','967'\n",
    "      )\n",
    "                   and t.CUSTOMER_ID IN (SELECT CUSTOMER FROM FilteredCustomers)\n",
    "),\n",
    "\n",
    "/* ==========================================\n",
    "   Month-Level Aggregation\n",
    "   ========================================== */\n",
    "\n",
    "Monthly AS (\n",
    "\n",
    "SELECT\n",
    "    SNAPSHOT_DATE,\n",
    "    CUSTOMER_ID,\n",
    "\n",
    "    /* Amount Buckets */\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=0  THEN AMOUNT_LCY ELSE 0 END) AS M1,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=1  THEN AMOUNT_LCY ELSE 0 END) AS M2,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=2  THEN AMOUNT_LCY ELSE 0 END) AS M3,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=3  THEN AMOUNT_LCY ELSE 0 END) AS M4,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=4  THEN AMOUNT_LCY ELSE 0 END) AS M5,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=5  THEN AMOUNT_LCY ELSE 0 END) AS M6,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=6  THEN AMOUNT_LCY ELSE 0 END) AS M7,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=7  THEN AMOUNT_LCY ELSE 0 END) AS M8,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=8  THEN AMOUNT_LCY ELSE 0 END) AS M9,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=9  THEN AMOUNT_LCY ELSE 0 END) AS M10,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=10 THEN AMOUNT_LCY ELSE 0 END) AS M11,\n",
    "    SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=11 THEN AMOUNT_LCY ELSE 0 END) AS M12,\n",
    "\n",
    "    /* Count Buckets */\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=0  THEN 1 END) AS C1,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=1  THEN 1 END) AS C2,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=2  THEN 1 END) AS C3,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=3  THEN 1 END) AS C4,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=4  THEN 1 END) AS C5,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=5  THEN 1 END) AS C6,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=6  THEN 1 END) AS C7,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=7  THEN 1 END) AS C8,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=8  THEN 1 END) AS C9,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=9  THEN 1 END) AS C10,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=10 THEN 1 END) AS C11,\n",
    "    COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=11 THEN 1 END) AS C12\n",
    "\n",
    "FROM BaseCredits\n",
    "GROUP BY SNAPSHOT_DATE, CUSTOMER_ID\n",
    ")\n",
    "\n",
    "/* ==========================================\n",
    "   Final Output With Trends\n",
    "   ========================================== */\n",
    "\n",
    "SELECT\n",
    "    SNAPSHOT_DATE,\n",
    "    CUSTOMER_ID,\n",
    "\n",
    "    /* Quarter Totals */\n",
    "    (M1+M2+M3)   AS Q1_CTO,\n",
    "    (M4+M5+M6)   AS Q2_CTO,\n",
    "    (M7+M8+M9)   AS Q3_CTO,\n",
    "    (M10+M11+M12) AS Q4_CTO,\n",
    "\n",
    "    (C1+C2+C3)   AS Q1_CTO_Count,\n",
    "    (C4+C5+C6)   AS Q2_CTO_Count,\n",
    "    (C7+C8+C9)   AS Q3_CTO_Count,\n",
    "    (C10+C11+C12) AS Q4_CTO_Count,\n",
    "\n",
    "    /* Month-to-Month Amount Trends */\n",
    "    CASE WHEN M2=0 THEN NULL ELSE ROUND((M1-M2)/NULLIF(M2,0),2) END AS M1M2_CTO_Amt_Change,\n",
    "    CASE WHEN M3=0 THEN NULL ELSE ROUND((M2-M3)/NULLIF(M3,0),2) END AS M2M3_CTO_Amt_Change,\n",
    "    CASE WHEN M4=0 THEN NULL ELSE ROUND((M3-M4)/NULLIF(M4,0),2) END AS M3M4_CTO_Amt_Change,\n",
    "    CASE WHEN M5=0 THEN NULL ELSE ROUND((M4-M5)/NULLIF(M5,0),2) END AS M4M5_CTO_Amt_Change,\n",
    "    CASE WHEN M6=0 THEN NULL ELSE ROUND((M5-M6)/NULLIF(M6,0),2) END AS M5M6_CTO_Amt_Change,\n",
    "    CASE WHEN M7=0 THEN NULL ELSE ROUND((M6-M7)/NULLIF(M7,0),2) END AS M6M7_CTO_Amt_Change,\n",
    "    CASE WHEN M8=0 THEN NULL ELSE ROUND((M7-M8)/NULLIF(M8,0),2) END AS M7M8_CTO_Amt_Change,\n",
    "    CASE WHEN M9=0 THEN NULL ELSE ROUND((M8-M9)/NULLIF(M9,0),2) END AS M8M9_CTO_Amt_Change,\n",
    "    CASE WHEN M10=0 THEN NULL ELSE ROUND((M9-M10)/NULLIF(M10,0),2) END AS M9M10_CTO_Amt_Change,\n",
    "    CASE WHEN M11=0 THEN NULL ELSE ROUND((M10-M11)/NULLIF(M11,0),2) END AS M10M11_CTO_Amt_Change,\n",
    "    CASE WHEN M12=0 THEN NULL ELSE ROUND((M11-M12)/NULLIF(M12,0),2) END AS M11M12_CTO_Amt_Change,\n",
    "\n",
    "    /* Quarter-to-Quarter Amount Trends */\n",
    "    CASE WHEN (M4+M5+M6)=0 THEN NULL\n",
    "         ELSE ROUND(((M1+M2+M3)-(M4+M5+M6)) / NULLIF((M4+M5+M6),0),2)\n",
    "    END AS Q1Q2_CTO_Amt_Change,\n",
    "\n",
    "    CASE WHEN (M7+M8+M9)=0 THEN NULL\n",
    "         ELSE ROUND(((M4+M5+M6)-(M7+M8+M9)) / NULLIF((M7+M8+M9),0),2)\n",
    "    END AS Q2Q3_CTO_Amt_Change,\n",
    "\n",
    "    CASE WHEN (M10+M11+M12)=0 THEN NULL\n",
    "         ELSE ROUND(((M7+M8+M9)-(M10+M11+M12)) / NULLIF((M10+M11+M12),0),2)\n",
    "    END AS Q3Q4_CTO_Amt_Change\n",
    "\n",
    "FROM Monthly\n",
    "ORDER BY SNAPSHOT_DATE, CUSTOMER_ID;\n",
    "\n",
    "                      \"\"\")\n",
    "cto_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNAPSHOT_DATE</th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>NO_OF_ACCOUNTS_FID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000388</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1001490</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1001810</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1002009</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1002465</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SNAPSHOT_DATE CUSTOMER_ID NO_OF_ACCOUNTS_FID\n",
       "0  2025-06-30 00:00:00     1000388                  1\n",
       "1  2025-06-30 00:00:00     1001490                  2\n",
       "2  2025-06-30 00:00:00     1001810                  1\n",
       "3  2025-06-30 00:00:00     1002009                  1\n",
       "4  2025-06-30 00:00:00     1002465                  1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FID_df = run_query(\"\"\" \n",
    "WITH SnapshotDates AS (\n",
    "    SELECT DISTINCT EXTRACTION_DATE AS SNAPSHOT_DATE \n",
    "    FROM dbcba.KE_Accounts_Master\n",
    "    WHERE EXTRACTION_DATE IN (\n",
    "        '2025-06-30','2025-07-31','2025-08-31',\n",
    "        '2025-09-30','2025-10-31','2025-11-30')\n",
    "                   )\n",
    ",\n",
    "FilteredCustomers AS (\n",
    "    SELECT am.CUSTOMER\n",
    "    FROM dbcba.KE_Accounts_List am\n",
    "    LEFT JOIN dbcba.KE_Customer_Master cm\n",
    "        ON cm.CUSTOMER_NUMBER = am.CUSTOMER\n",
    "    WHERE am.PRODUCT_LINE='LENDING'\n",
    "      AND am.ARR_STATUS='CURRENT'\n",
    "      AND cm.BUSINESS_SEGMENT IN ('270','250')\n",
    "      AND COALESCE(am.ONLINE_ACTUAL_BAL,0) < 0\n",
    "      AND am.CUSTOMER IS NOT NULL\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    s.SNAPSHOT_DATE,\n",
    "    a.customer_number AS CUSTOMER_ID,\n",
    "    COUNT(DISTINCT a.ACCOUNT_NUMBER) AS NO_OF_ACCOUNTS_FID\n",
    "FROM SnapshotDates s\n",
    "JOIN STGKE.STG_AA_ARREARS a\n",
    "    ON a.EXTRACTION_DATE <= s.SNAPSHOT_DATE\n",
    "JOIN DBCBA.KE_ACCOUNTS_LIST b\n",
    "    ON a.ACCOUNT_NUMBER = b.ACCOUNT_NUMBER\n",
    "WHERE a.Days_In_Arrears BETWEEN 1 AND 30\n",
    "AND DATEDIFF(DAY,b.ARR_START_DATE,a.ARREARS_DATE) BETWEEN 1 AND 30\n",
    "and a.customer_number in (SELECT CUSTOMER FROM FilteredCustomers)\n",
    "GROUP BY s.SNAPSHOT_DATE, a.customer_number;\n",
    "\n",
    "\"\"\")\n",
    "FID_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNAPSHOT_DATE</th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>Q1_Unpaid_Value</th>\n",
       "      <th>Q2_Unpaid_Value</th>\n",
       "      <th>Q3_Unpaid_Value</th>\n",
       "      <th>Q4_Unpaid_Value</th>\n",
       "      <th>Q1_Unpaid_Counts</th>\n",
       "      <th>Q2_Unpaid_Counts</th>\n",
       "      <th>Q3_Unpaid_Counts</th>\n",
       "      <th>Q4_Unpaid_Counts</th>\n",
       "      <th>Q1Q2_Unpaid_Amt_change_pct</th>\n",
       "      <th>Q2Q3_Unpaid_Amt_change_pct</th>\n",
       "      <th>Q3Q4_Unpaid_Amt_change_pct</th>\n",
       "      <th>Q1Q2_Unpaid_Cnt_Change_pct</th>\n",
       "      <th>Q2Q3_Unpaid_Cnt_Change_pct</th>\n",
       "      <th>Q3Q4_Unpaid_Cnt_Change_pct</th>\n",
       "      <th>CurrMonthPrevMonth_Unpaid_Amt_change_pct</th>\n",
       "      <th>CurrMonthPrevMonth_Unpaid_Cnt_change_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>219,765.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000769</td>\n",
       "      <td>30,229.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1001558</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>340,000.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1001752</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>325,000.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1003502</td>\n",
       "      <td>50,447.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SNAPSHOT_DATE CUSTOMER_ID  Q1_Unpaid_Value  Q2_Unpaid_Value  \\\n",
       "0  2025-06-30 00:00:00     1000272           0.0000     219,765.0000   \n",
       "1  2025-06-30 00:00:00     1000769      30,229.0000           0.0000   \n",
       "2  2025-06-30 00:00:00     1001558           0.0000     340,000.0000   \n",
       "3  2025-06-30 00:00:00     1001752           0.0000     325,000.0000   \n",
       "4  2025-06-30 00:00:00     1003502      50,447.0000           0.0000   \n",
       "\n",
       "   Q3_Unpaid_Value  Q4_Unpaid_Value  Q1_Unpaid_Counts  Q2_Unpaid_Counts  \\\n",
       "0           0.0000           0.0000                 0                 1   \n",
       "1           0.0000           0.0000                 1                 0   \n",
       "2           0.0000           0.0000                 0                 1   \n",
       "3           0.0000           0.0000                 0                 1   \n",
       "4           0.0000           0.0000                 1                 0   \n",
       "\n",
       "   Q3_Unpaid_Counts  Q4_Unpaid_Counts  Q1Q2_Unpaid_Amt_change_pct  \\\n",
       "0                 0                 0                     -1.0000   \n",
       "1                 0                 0                         NaN   \n",
       "2                 0                 0                     -1.0000   \n",
       "3                 0                 0                     -1.0000   \n",
       "4                 0                 0                         NaN   \n",
       "\n",
       "   Q2Q3_Unpaid_Amt_change_pct  Q3Q4_Unpaid_Amt_change_pct  \\\n",
       "0                         NaN                         NaN   \n",
       "1                         NaN                         NaN   \n",
       "2                         NaN                         NaN   \n",
       "3                         NaN                         NaN   \n",
       "4                         NaN                         NaN   \n",
       "\n",
       "   Q1Q2_Unpaid_Cnt_Change_pct  Q2Q3_Unpaid_Cnt_Change_pct  \\\n",
       "0                     -1.0000                         NaN   \n",
       "1                         NaN                         NaN   \n",
       "2                     -1.0000                         NaN   \n",
       "3                     -1.0000                         NaN   \n",
       "4                         NaN                         NaN   \n",
       "\n",
       "   Q3Q4_Unpaid_Cnt_Change_pct  CurrMonthPrevMonth_Unpaid_Amt_change_pct  \\\n",
       "0                         NaN                                       NaN   \n",
       "1                         NaN                                       NaN   \n",
       "2                         NaN                                       NaN   \n",
       "3                         NaN                                       NaN   \n",
       "4                         NaN                                       NaN   \n",
       "\n",
       "   CurrMonthPrevMonth_Unpaid_Cnt_change_pct  \n",
       "0                                       NaN  \n",
       "1                                       NaN  \n",
       "2                                       NaN  \n",
       "3                                       NaN  \n",
       "4                                       NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpaid_df = run_query(\"\"\"\n",
    "-- ================================================\n",
    "-- Full Snapshot-Aligned Unpaid Feature Script\n",
    "-- ================================================\n",
    "\n",
    "WITH SnapshotDates AS (\n",
    "    -- Step 1: Define your snapshot dates (as per model requirements)\n",
    "    SELECT DISTINCT EXTRACTION_DATE AS SNAPSHOT_DATE\n",
    "    FROM dbcba.KE_Accounts_Master\n",
    "    WHERE EXTRACTION_DATE IN (\n",
    "        '2025-06-30','2025-07-31','2025-08-31',\n",
    "        '2025-09-30','2025-10-31','2025-11-30'\n",
    "    )\n",
    "),\n",
    "FilteredCustomers AS (\n",
    "    SELECT am.CUSTOMER\n",
    "    FROM dbcba.KE_Accounts_List am\n",
    "    LEFT JOIN dbcba.KE_Customer_Master cm\n",
    "        ON cm.CUSTOMER_NUMBER = am.CUSTOMER\n",
    "    WHERE am.PRODUCT_LINE='LENDING'\n",
    "      AND am.ARR_STATUS='CURRENT'\n",
    "      AND cm.BUSINESS_SEGMENT IN ('270','250')\n",
    "      AND COALESCE(am.ONLINE_ACTUAL_BAL,0) < 0\n",
    "      AND am.CUSTOMER IS NOT NULL\n",
    "),\n",
    "\n",
    "BaseUnpaid AS (\n",
    "    -- Step 2: Pull all unpaid transactions in the last 12 months relative to snapshot\n",
    "    SELECT\n",
    "        s.SNAPSHOT_DATE,\n",
    "        t.CUSTOMER_ID,\n",
    "        t.BOOKING_DATE,\n",
    "        CAST(t.AMOUNT_LCY * -1 AS DECIMAL(18,2)) AS UNPAID_AMOUNT\n",
    "    FROM SnapshotDates s\n",
    "    JOIN dbcba.KE_ACCOUNT_TRANSACTIONS t\n",
    "        ON t.BOOKING_DATE <= s.SNAPSHOT_DATE\n",
    "       AND t.BOOKING_DATE > DATEADD(MONTH,-12,s.SNAPSHOT_DATE)\n",
    "    WHERE t.AMOUNT_LCY < 0\n",
    "      AND (t.DEBIT_CUSTOMER <> t.CREDIT_CUSTOMER OR t.CREDIT_CUSTOMER IS NULL)\n",
    "      AND t.TRANSACTION_CODE IN (\n",
    "          '938','939','940','941','944','945','946','947','948','949',\n",
    "          '991','992','1084','1085','285','561','882','904','905','923'\n",
    "      )\n",
    "      AND t.REVERSAL_MARKER <> 'R'\n",
    "                      and t.CUSTOMER_ID IN (SELECT CUSTOMER FROM FilteredCustomers)\n",
    "),\n",
    "\n",
    "Monthly AS (\n",
    "    -- Step 3: Bucket transactions into 12 months relative to snapshot\n",
    "    SELECT\n",
    "        SNAPSHOT_DATE,\n",
    "        CUSTOMER_ID,\n",
    "\n",
    "        -- Amounts per month\n",
    "        SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=0 THEN UNPAID_AMOUNT ELSE 0 END) AS M1,\n",
    "        SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=1 THEN UNPAID_AMOUNT ELSE 0 END) AS M2,\n",
    "        SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=2 THEN UNPAID_AMOUNT ELSE 0 END) AS M3,\n",
    "        SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=3 THEN UNPAID_AMOUNT ELSE 0 END) AS M4,\n",
    "        SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=4 THEN UNPAID_AMOUNT ELSE 0 END) AS M5,\n",
    "        SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=5 THEN UNPAID_AMOUNT ELSE 0 END) AS M6,\n",
    "        SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=6 THEN UNPAID_AMOUNT ELSE 0 END) AS M7,\n",
    "        SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=7 THEN UNPAID_AMOUNT ELSE 0 END) AS M8,\n",
    "        SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=8 THEN UNPAID_AMOUNT ELSE 0 END) AS M9,\n",
    "        SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=9 THEN UNPAID_AMOUNT ELSE 0 END) AS M10,\n",
    "        SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=10 THEN UNPAID_AMOUNT ELSE 0 END) AS M11,\n",
    "        SUM(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=11 THEN UNPAID_AMOUNT ELSE 0 END) AS M12,\n",
    "\n",
    "        -- Counts per month\n",
    "        COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=0 THEN 1 END) AS C1,\n",
    "        COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=1 THEN 1 END) AS C2,\n",
    "        COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=2 THEN 1 END) AS C3,\n",
    "        COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=3 THEN 1 END) AS C4,\n",
    "        COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=4 THEN 1 END) AS C5,\n",
    "        COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=5 THEN 1 END) AS C6,\n",
    "        COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=6 THEN 1 END) AS C7,\n",
    "        COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=7 THEN 1 END) AS C8,\n",
    "        COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=8 THEN 1 END) AS C9,\n",
    "        COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=9 THEN 1 END) AS C10,\n",
    "        COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=10 THEN 1 END) AS C11,\n",
    "        COUNT(CASE WHEN DATEDIFF(MONTH, BOOKING_DATE, SNAPSHOT_DATE)=11 THEN 1 END) AS C12\n",
    "\n",
    "    FROM BaseUnpaid\n",
    "    GROUP BY SNAPSHOT_DATE, CUSTOMER_ID\n",
    "),\n",
    "\n",
    "FinalUnpaid AS (\n",
    "    -- Step 4: Calculate quarterly aggregates and trends\n",
    "    SELECT\n",
    "        SNAPSHOT_DATE,\n",
    "        CUSTOMER_ID,\n",
    "\n",
    "        -- Quarter values\n",
    "        (M1+M2+M3)   AS Q1_Unpaid_Value,\n",
    "        (M4+M5+M6)   AS Q2_Unpaid_Value,\n",
    "        (M7+M8+M9)   AS Q3_Unpaid_Value,\n",
    "        (M10+M11+M12) AS Q4_Unpaid_Value,\n",
    "\n",
    "        (C1+C2+C3)   AS Q1_Unpaid_Counts,\n",
    "        (C4+C5+C6)   AS Q2_Unpaid_Counts,\n",
    "        (C7+C8+C9)   AS Q3_Unpaid_Counts,\n",
    "        (C10+C11+C12) AS Q4_Unpaid_Counts,\n",
    "\n",
    "        -- Quarter amount trends\n",
    "        CASE WHEN (M4+M5+M6)=0 THEN NULL\n",
    "             ELSE ROUND(((M1+M2+M3)-(M4+M5+M6))/NULLIF((M4+M5+M6),0),2)\n",
    "        END AS Q1Q2_Unpaid_Amt_change_pct,\n",
    "\n",
    "        CASE WHEN (M7+M8+M9)=0 THEN NULL\n",
    "             ELSE ROUND(((M4+M5+M6)-(M7+M8+M9))/NULLIF((M7+M8+M9),0),2)\n",
    "        END AS Q2Q3_Unpaid_Amt_change_pct,\n",
    "\n",
    "        CASE WHEN (M10+M11+M12)=0 THEN NULL\n",
    "             ELSE ROUND(((M7+M8+M9)-(M10+M11+M12))/NULLIF((M10+M11+M12),0),2)\n",
    "        END AS Q3Q4_Unpaid_Amt_change_pct,\n",
    "\n",
    "        -- Quarter count trends\n",
    "        CASE WHEN (C4+C5+C6)=0 THEN NULL\n",
    "             ELSE ROUND(((C1+C2+C3)-(C4+C5+C6))/NULLIF((C4+C5+C6),0),2)\n",
    "        END AS Q1Q2_Unpaid_Cnt_Change_pct,\n",
    "\n",
    "        CASE WHEN (C7+C8+C9)=0 THEN NULL\n",
    "             ELSE ROUND(((C4+C5+C6)-(C7+C8+C9))/NULLIF((C7+C8+C9),0),2)\n",
    "        END AS Q2Q3_Unpaid_Cnt_Change_pct,\n",
    "\n",
    "        CASE WHEN (C10+C11+C12)=0 THEN NULL\n",
    "             ELSE ROUND(((C7+C8+C9)-(C10+C11+C12))/NULLIF((C10+C11+C12),0),2)\n",
    "        END AS Q3Q4_Unpaid_Cnt_Change_pct,\n",
    "\n",
    "        -- Current vs previous month\n",
    "        CASE WHEN M2=0 THEN NULL\n",
    "             ELSE ROUND((M1-M2)/NULLIF(M2,0),2)\n",
    "        END AS CurrMonthPrevMonth_Unpaid_Amt_change_pct,\n",
    "\n",
    "        CASE WHEN C2=0 THEN NULL\n",
    "             ELSE ROUND((C1-C2)/NULLIF(C2,0),2)\n",
    "        END AS CurrMonthPrevMonth_Unpaid_Cnt_change_pct\n",
    "\n",
    "    FROM Monthly\n",
    ")\n",
    "\n",
    "-- Final output\n",
    "SELECT *\n",
    "FROM FinalUnpaid\n",
    "where CUSTOMER_ID IS NOT NULL\n",
    "ORDER BY SNAPSHOT_DATE, CUSTOMER_ID;\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "unpaid_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNAPSHOT_DATE</th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>MAX_LOAN_DPD_60D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000388</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000451</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000626</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000775</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1001198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SNAPSHOT_DATE CUSTOMER_ID  MAX_LOAN_DPD_60D\n",
       "0  2025-06-30 00:00:00     1000388                 0\n",
       "1  2025-06-30 00:00:00     1000451                 0\n",
       "2  2025-06-30 00:00:00     1000626                 0\n",
       "3  2025-06-30 00:00:00     1000775                 0\n",
       "4  2025-06-30 00:00:00     1001198                 0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxdpd_df = run_query(\"\"\" \n",
    "WITH SnapshotDates AS (\n",
    "    SELECT DISTINCT EXTRACTION_DATE AS SNAPSHOT_DATE \n",
    "    FROM dbcba.KE_Accounts_Master\n",
    "    WHERE EXTRACTION_DATE IN (\n",
    "        '2025-06-30','2025-07-31','2025-08-31',\n",
    "        '2025-09-30','2025-10-31','2025-11-30')),\n",
    "                      \n",
    "FilteredCustomers AS (\n",
    "    SELECT am.CUSTOMER\n",
    "    FROM dbcba.KE_Accounts_List am\n",
    "    LEFT JOIN dbcba.KE_Customer_Master cm\n",
    "        ON cm.CUSTOMER_NUMBER = am.CUSTOMER\n",
    "    WHERE am.PRODUCT_LINE='LENDING'\n",
    "      AND am.ARR_STATUS='CURRENT'\n",
    "      AND cm.BUSINESS_SEGMENT IN ('270','250')\n",
    "      AND COALESCE(am.ONLINE_ACTUAL_BAL,0) < 0\n",
    "      AND am.CUSTOMER IS NOT NULL\n",
    ")                   \n",
    "SELECT\n",
    "    s.SNAPSHOT_DATE,\n",
    "    a.CUSTOMER AS CUSTOMER_ID,\n",
    "    MAX(a.OD_DAYS) AS MAX_LOAN_DPD_60D\n",
    "FROM SnapshotDates s\n",
    "JOIN STGKE.STG_AA_BILL_ARREARS a\n",
    "    ON a.EXTRACTION_DATE > DATEADD(DAY,-60,s.SNAPSHOT_DATE)\n",
    "    AND a.EXTRACTION_DATE <= s.SNAPSHOT_DATE\n",
    "                      and  a.CUSTOMER in (SELECT CUSTOMER FROM FilteredCustomers)\n",
    "GROUP BY s.SNAPSHOT_DATE, a.CUSTOMER;\n",
    "\"\"\")\n",
    "maxdpd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNAPSHOT_DATE</th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>NUMBEROF_ACCOUNTS</th>\n",
       "      <th>OPENACCOUNT</th>\n",
       "      <th>PERFORMING</th>\n",
       "      <th>NONPERFORMING</th>\n",
       "      <th>CLOSED</th>\n",
       "      <th>CURRENTINARREARS</th>\n",
       "      <th>ARREARS0DAYS</th>\n",
       "      <th>MAXARREARS</th>\n",
       "      <th>MAXARREARSLAST12MONTHS</th>\n",
       "      <th>MAXARREARSLAST3MONTHS</th>\n",
       "      <th>PRINCIPALAMOUNT</th>\n",
       "      <th>SCHEDULEDPAYMENTAMT</th>\n",
       "      <th>CREDITAPPLICATIONSDONE</th>\n",
       "      <th>CURRENTBALANCEAMOUNT</th>\n",
       "      <th>SUBSCRIBERCURRENTBALANCEAMOUNT</th>\n",
       "      <th>PASTDUEAMOUNT</th>\n",
       "      <th>NUMBEROFENQUIRIES</th>\n",
       "      <th>ENQUIRIES30DAYS</th>\n",
       "      <th>ENQUIRIES90DAYS</th>\n",
       "      <th>CRB_TU_SCORE</th>\n",
       "      <th>MOBILE_TOTAL</th>\n",
       "      <th>MOBILE_PERFORMING</th>\n",
       "      <th>MOBILE_NONPERFORMING</th>\n",
       "      <th>MOBILE_PRINCIPALAMOUNT</th>\n",
       "      <th>LOAD_DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000003</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>321307.27</td>\n",
       "      <td>324303.49</td>\n",
       "      <td>0</td>\n",
       "      <td>27378.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>739</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>318186</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000009</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132319.37</td>\n",
       "      <td>133106.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>677</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10500</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000011</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1341751.98</td>\n",
       "      <td>1445082.25</td>\n",
       "      <td>0</td>\n",
       "      <td>25590.33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>710</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>1336000</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000012</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>534</td>\n",
       "      <td>534</td>\n",
       "      <td>0</td>\n",
       "      <td>17601.4</td>\n",
       "      <td>15858.24</td>\n",
       "      <td>0</td>\n",
       "      <td>9863.56</td>\n",
       "      <td>863.56</td>\n",
       "      <td>9000</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>566</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7100</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06-30 00:00:00</td>\n",
       "      <td>1000014</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>1719</td>\n",
       "      <td>1719</td>\n",
       "      <td>1719</td>\n",
       "      <td>164009.15</td>\n",
       "      <td>139209.55</td>\n",
       "      <td>0</td>\n",
       "      <td>45556.33</td>\n",
       "      <td>2752.09</td>\n",
       "      <td>42804.24</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>524</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>49700</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SNAPSHOT_DATE CUSTOMER_ID NUMBEROF_ACCOUNTS OPENACCOUNT PERFORMING  \\\n",
       "0  2025-06-30 00:00:00     1000003                55           2         55   \n",
       "1  2025-06-30 00:00:00     1000009                32           0         32   \n",
       "2  2025-06-30 00:00:00     1000011                95           1         95   \n",
       "3  2025-06-30 00:00:00     1000012                22           1         21   \n",
       "4  2025-06-30 00:00:00     1000014                40           2         38   \n",
       "\n",
       "  NONPERFORMING CLOSED CURRENTINARREARS ARREARS0DAYS MAXARREARS  \\\n",
       "0             0     53                0           55          0   \n",
       "1             0     32                0           32          0   \n",
       "2             0     94                0           95          0   \n",
       "3             1     20                1           21        534   \n",
       "4             2     37                2           38       1719   \n",
       "\n",
       "  MAXARREARSLAST12MONTHS MAXARREARSLAST3MONTHS PRINCIPALAMOUNT  \\\n",
       "0                      0                     0       321307.27   \n",
       "1                      0                     0       132319.37   \n",
       "2                      0                     0      1341751.98   \n",
       "3                    534                     0         17601.4   \n",
       "4                   1719                  1719       164009.15   \n",
       "\n",
       "  SCHEDULEDPAYMENTAMT CREDITAPPLICATIONSDONE CURRENTBALANCEAMOUNT  \\\n",
       "0           324303.49                      0              27378.5   \n",
       "1           133106.87                      0                    0   \n",
       "2          1445082.25                      0             25590.33   \n",
       "3            15858.24                      0              9863.56   \n",
       "4           139209.55                      0             45556.33   \n",
       "\n",
       "  SUBSCRIBERCURRENTBALANCEAMOUNT PASTDUEAMOUNT NUMBEROFENQUIRIES  \\\n",
       "0                              0             0                 4   \n",
       "1                              0             0                 7   \n",
       "2                              0             0                 9   \n",
       "3                         863.56          9000                19   \n",
       "4                        2752.09      42804.24                34   \n",
       "\n",
       "  ENQUIRIES30DAYS ENQUIRIES90DAYS CRB_TU_SCORE MOBILE_TOTAL MOBILE_PERFORMING  \\\n",
       "0               0               1          739           48                48   \n",
       "1               0               0          677            1                 1   \n",
       "2               0               0          710           82                82   \n",
       "3               0               2          566            3                 2   \n",
       "4               0               0          524            5                 4   \n",
       "\n",
       "  MOBILE_NONPERFORMING MOBILE_PRINCIPALAMOUNT   LOAD_DATE  \n",
       "0                    0                 318186  2025-06-25  \n",
       "1                    0                  10500  2025-06-25  \n",
       "2                    0                1336000  2025-06-25  \n",
       "3                    1                   7100  2025-06-25  \n",
       "4                    1                  49700  2025-06-25  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crb_df = run_query(\"\"\" \n",
    "                            \n",
    "WITH SnapshotDates AS (\n",
    "    SELECT DISTINCT EXTRACTION_DATE AS SNAPSHOT_DATE\n",
    "    FROM dbcba.KE_Accounts_Master\n",
    "    WHERE EXTRACTION_DATE IN (\n",
    "        '2025-06-30','2025-07-31','2025-08-31',\n",
    "        '2025-09-30','2025-10-31','2025-11-30'\n",
    "    )\n",
    "),\n",
    "FilteredCustomers AS (\n",
    "    SELECT am.CUSTOMER\n",
    "    FROM dbcba.KE_Accounts_List am\n",
    "    LEFT JOIN dbcba.KE_Customer_Master cm\n",
    "        ON cm.CUSTOMER_NUMBER = am.CUSTOMER\n",
    "    WHERE am.PRODUCT_LINE='LENDING'\n",
    "      AND am.ARR_STATUS='CURRENT'\n",
    "      AND cm.BUSINESS_SEGMENT IN ('270','250')\n",
    "      AND COALESCE(am.ONLINE_ACTUAL_BAL,0) < 0\n",
    "      AND am.CUSTOMER IS NOT NULL\n",
    "),\n",
    "LatestCRB AS (\n",
    "    SELECT\n",
    "        s.SNAPSHOT_DATE,\n",
    "        c.CLIENTID AS CUSTOMER_ID,  -- keep as string\n",
    "        c.NUMBEROF_ACCOUNTS,\n",
    "        c.OPENACCOUNT,\n",
    "        c.PERFORMING,\n",
    "        c.NONPERFORMING,\n",
    "        c.CLOSED,\n",
    "        c.CURRENTINARREARS,\n",
    "        c.ARREARS0DAYS,\n",
    "        c.MAXARREARS,\n",
    "        c.MAXARREARSLAST12MONTHS,\n",
    "        c.MAXARREARSLAST3MONTHS,\n",
    "        c.PRINCIPALAMOUNT,\n",
    "        c.SCHEDULEDPAYMENTAMT,\n",
    "        c.CREDITAPPLICATIONSDONE,\n",
    "        c.CURRENTBALANCEAMOUNT,\n",
    "        c.SUBSCRIBERCURRENTBALANCEAMOUNT,\n",
    "        c.PASTDUEAMOUNT,\n",
    "        c.NUMBEROFENQUIRIES,\n",
    "        c.ENQUIRIES30DAYS,\n",
    "        c.ENQUIRIES90DAYS,\n",
    "        c.SCORE AS CRB_TU_SCORE,\n",
    "        c.MOBILE_TOTAL,\n",
    "        c.MOBILE_PERFORMING,\n",
    "        c.MOBILE_NONPERFORMING,\n",
    "        c.MOBILE_PRINCIPALAMOUNT,\n",
    "        c.LOAD_DATE\n",
    "    FROM SnapshotDates s\n",
    "    JOIN dbcba.LEND_FOR_ALL_CRB_DATA c\n",
    "      ON c.LOAD_DATE <= s.SNAPSHOT_DATE\n",
    "    WHERE ISNUMERIC(c.CLIENTID) = 1  -- filter numeric only\n",
    "                   and c.CLIENTID IN (SELECT CUSTOMER FROM FilteredCustomers)\n",
    "),\n",
    "\n",
    "CRB_Max AS (\n",
    "    SELECT l.*\n",
    "    FROM LatestCRB l\n",
    "    JOIN (\n",
    "        SELECT \n",
    "            SNAPSHOT_DATE,\n",
    "            CUSTOMER_ID,\n",
    "            MAX(LOAD_DATE) AS MAX_LOAD_DATE\n",
    "        FROM LatestCRB\n",
    "        GROUP BY SNAPSHOT_DATE, CUSTOMER_ID\n",
    "    ) m\n",
    "      ON l.SNAPSHOT_DATE = m.SNAPSHOT_DATE\n",
    "     AND l.CUSTOMER_ID = m.CUSTOMER_ID\n",
    "     AND l.LOAD_DATE = m.MAX_LOAD_DATE\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM CRB_Max\n",
    "ORDER BY SNAPSHOT_DATE, CUSTOMER_ID;\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "crb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Pull Filtered Customers\n",
    "\n",
    "filtered_customers_df = run_query(\"\"\"\n",
    "SELECT DISTINCT am.CUSTOMER AS CUSTOMER_ID\n",
    "FROM dbcba.KE_Accounts_List am\n",
    "INNER JOIN dbcba.KE_Customer_Master cm\n",
    "    ON cm.CUSTOMER_NUMBER = am.CUSTOMER\n",
    "WHERE am.PRODUCT_LINE='LENDING'\n",
    "  AND am.ARR_STATUS='CURRENT'\n",
    "  AND cm.BUSINESS_SEGMENT IN ('270','250')\n",
    "  AND COALESCE(am.ONLINE_ACTUAL_BAL,0) < 0\n",
    "  AND am.CUSTOMER IS NOT NULL\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_bal_df = run_query(\"\"\"\n",
    "SELECT\n",
    "    acc.EXTRACTION_DATE AS SNAPSHOT_DATE,\n",
    "    acc.CUSTOMER AS CUSTOMER_ID,\n",
    "    avg_bal.Av_Bal_Date,\n",
    "    avg_bal.CUMULATIVE_BAL_LCY,\n",
    "    avg_bal.NUM_DAYS\n",
    "FROM dbcba.KE_Accounts_master acc\n",
    "INNER JOIN (\n",
    "    SELECT DISTINCT am.CUSTOMER\n",
    "    FROM dbcba.KE_Accounts_List am\n",
    "    INNER JOIN dbcba.KE_Customer_Master cm\n",
    "        ON cm.CUSTOMER_NUMBER = am.CUSTOMER\n",
    "    WHERE am.PRODUCT_LINE='LENDING'\n",
    "      AND am.ARR_STATUS='CURRENT'\n",
    "      AND cm.BUSINESS_SEGMENT IN ('270','250')\n",
    "      AND COALESCE(am.ONLINE_ACTUAL_BAL,0) < 0\n",
    "      AND am.CUSTOMER IS NOT NULL\n",
    ") fc\n",
    "    ON acc.CUSTOMER = fc.CUSTOMER\n",
    "LEFT JOIN STGKE.STG_AVERAGE_BALANCE avg_bal\n",
    "    ON avg_bal.Account_number = acc.Account_number\n",
    "    AND avg_bal.Av_Bal_Date <= acc.EXTRACTION_DATE\n",
    "    AND avg_bal.Av_Bal_Date > DATEADD(MONTH,-12,acc.EXTRACTION_DATE)\n",
    "WHERE acc.PRODUCT_LINE='ACCOUNTS'\n",
    "AND acc.EXTRACTION_DATE IN (\n",
    "    '2025-06-30','2025-07-31','2025-08-31',\n",
    "    '2025-09-30','2025-10-31','2025-11-30'\n",
    ")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_bal_df[\"MONTH_DIFF\"] = (\n",
    "    (pd.to_datetime(base_bal_df[\"SNAPSHOT_DATE\"]).dt.year -\n",
    "     pd.to_datetime(base_bal_df[\"Av_Bal_Date\"]).dt.year) * 12\n",
    "    +\n",
    "    (pd.to_datetime(base_bal_df[\"SNAPSHOT_DATE\"]).dt.month -\n",
    "     pd.to_datetime(base_bal_df[\"Av_Bal_Date\"]).dt.month)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "base_bal_df[\"SNAPSHOT_DATE\"] = pd.to_datetime(base_bal_df[\"SNAPSHOT_DATE\"])\n",
    "base_bal_df[\"Av_Bal_Date\"] = pd.to_datetime(base_bal_df[\"Av_Bal_Date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_bal_df[\"MONTH_DIFF\"] = (\n",
    "    (base_bal_df[\"SNAPSHOT_DATE\"].dt.year - base_bal_df[\"Av_Bal_Date\"].dt.year) * 12\n",
    "    +\n",
    "    (base_bal_df[\"SNAPSHOT_DATE\"].dt.month - base_bal_df[\"Av_Bal_Date\"].dt.month)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_pivot = (\n",
    "    monthly_df\n",
    "    .pivot_table(\n",
    "        index=[\"CUSTOMER_ID\", \"SNAPSHOT_DATE\"],\n",
    "        columns=\"MONTH_DIFF\",\n",
    "        values=\"CUMULATIVE_BAL_LCY\",\n",
    "        fill_value=0\n",
    "    )\n",
    ")\n",
    "\n",
    "monthly_pivot.columns = [f\"M{int(col)+1}\" for col in monthly_pivot.columns]\n",
    "monthly_pivot = monthly_pivot.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 13):\n",
    "    col = f\"M{i}\"\n",
    "    if col not in monthly_pivot.columns:\n",
    "        monthly_pivot[col] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_bal_df = (\n",
    "    monthly_pivot\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "avg_bal_df[\"AVG_BAL_12M\"] = (\n",
    "    avg_bal_df[[f\"M{i}\" for i in range(1,13)]].sum(axis=1) / 12\n",
    ")\n",
    "\n",
    "avg_bal_df = avg_bal_df[[\"CUSTOMER_ID\", \"SNAPSHOT_DATE\", \"AVG_BAL_12M\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = avg_bal_df.merge(\n",
    "    monthly_pivot,\n",
    "    on=[\"CUSTOMER_ID\", \"SNAPSHOT_DATE\"],\n",
    "    how=\"left\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 12):\n",
    "    col1 = f\"M{i}\"\n",
    "    col2 = f\"M{i+1}\"\n",
    "    new_col = f\"M{i}M{i+1}_Change\"\n",
    "    \n",
    "    final_df[new_col] = (\n",
    "        (final_df[col1] - final_df[col2]) /\n",
    "        final_df[col2].replace(0, pd.NA)\n",
    "    ).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"Q1Q2_Change\"] = (\n",
    "    (final_df[\"M1\"] + final_df[\"M2\"] + final_df[\"M3\"] -\n",
    "     final_df[\"M4\"] - final_df[\"M5\"] - final_df[\"M6\"]) /\n",
    "    (final_df[\"M4\"] + final_df[\"M5\"] + final_df[\"M6\"]).replace(0, pd.NA)\n",
    ").round(2)\n",
    "\n",
    "final_df[\"Q2Q3_Change\"] = (\n",
    "    (final_df[\"M4\"] + final_df[\"M5\"] + final_df[\"M6\"] -\n",
    "     final_df[\"M7\"] - final_df[\"M8\"] - final_df[\"M9\"]) /\n",
    "    (final_df[\"M7\"] + final_df[\"M8\"] + final_df[\"M9\"]).replace(0, pd.NA)\n",
    ").round(2)\n",
    "\n",
    "final_df[\"Q3Q4_Change\"] = (\n",
    "    (final_df[\"M7\"] + final_df[\"M8\"] + final_df[\"M9\"] -\n",
    "     final_df[\"M10\"] - final_df[\"M11\"] - final_df[\"M12\"]) /\n",
    "    (final_df[\"M10\"] + final_df[\"M11\"] + final_df[\"M12\"]).replace(0, pd.NA)\n",
    ").round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cols = [\"CUSTOMER_ID\", \"SNAPSHOT_DATE\"]\n",
    "avg_col = [\"AVG_BAL_12M\"]\n",
    "month_cols = [f\"M{i}\" for i in range(1,13)]\n",
    "trend_cols = [col for col in final_df.columns if \"Change\" in col]\n",
    "\n",
    "final_df = final_df[base_cols + avg_col + month_cols + trend_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc_df = run_query(\"\"\"\n",
    "WITH SnapshotDates AS (\n",
    "    SELECT DISTINCT EXTRACTION_DATE AS SNAPSHOT_DATE\n",
    "    FROM dbcba.KE_Accounts_Master\n",
    "    WHERE EXTRACTION_DATE IN (\n",
    "        '2025-06-30','2025-07-31','2025-08-31',\n",
    "        '2025-09-30','2025-10-31','2025-11-30'\n",
    "    )\n",
    "),\n",
    "                   \n",
    "FilteredCustomers AS (\n",
    "    SELECT am.CUSTOMER\n",
    "    FROM dbcba.KE_Accounts_List am\n",
    "    LEFT JOIN dbcba.KE_Customer_Master cm\n",
    "        ON cm.CUSTOMER_NUMBER = am.CUSTOMER\n",
    "    WHERE am.PRODUCT_LINE='LENDING'\n",
    "      AND am.ARR_STATUS='CURRENT'\n",
    "      AND cm.BUSINESS_SEGMENT IN ('270','250')\n",
    "      AND COALESCE(am.ONLINE_ACTUAL_BAL,0) < 0\n",
    "      AND am.CUSTOMER IS NOT NULL\n",
    "),\n",
    "LatestPPC AS (\n",
    "    SELECT\n",
    "        p.CUSTOMER,\n",
    "        s.SNAPSHOT_DATE,\n",
    "        MAX(p.LOAD_DATE) AS MAX_LOAD_DATE\n",
    "    FROM dbcba.RETAIL_PPC p\n",
    "    JOIN SnapshotDates s\n",
    "      ON p.LOAD_DATE <= s.SNAPSHOT_DATE\n",
    "    AND p.CUSTOMER IN (SELECT CUSTOMER FROM FilteredCustomers)\n",
    "    GROUP BY p.CUSTOMER, s.SNAPSHOT_DATE\n",
    ")\n",
    "SELECT\n",
    "    s.SNAPSHOT_DATE,\n",
    "    l.CUSTOMER AS CUSTOMER_ID,\n",
    "    r.PPC\n",
    "FROM SnapshotDates s\n",
    "LEFT JOIN LatestPPC l\n",
    "       ON s.SNAPSHOT_DATE = l.SNAPSHOT_DATE\n",
    "LEFT JOIN dbcba.RETAIL_PPC r\n",
    "       ON r.CUSTOMER = l.CUSTOMER\n",
    "      AND r.LOAD_DATE = l.MAX_LOAD_DATE\n",
    "ORDER BY s.SNAPSHOT_DATE, l.CUSTOMER;\n",
    "\"\"\")\n",
    "ppc_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_df = run_query(\"\"\"WITH SnapshotDates AS (\n",
    "    SELECT DISTINCT EXTRACTION_DATE AS SNAPSHOT_DATE\n",
    "    FROM dbcba.KE_Accounts_Master\n",
    "    WHERE EXTRACTION_DATE IN (\n",
    "        '2025-06-30','2025-07-31','2025-08-31',\n",
    "        '2025-09-30','2025-10-31','2025-11-30'\n",
    "    )\n",
    ")\n",
    "FilteredCustomers AS (\n",
    "    SELECT am.CUSTOMER\n",
    "    FROM dbcba.KE_Accounts_List am\n",
    "    LEFT JOIN dbcba.KE_Customer_Master cm\n",
    "        ON cm.CUSTOMER_NUMBER = am.CUSTOMER\n",
    "    WHERE am.PRODUCT_LINE='LENDING'\n",
    "      AND am.ARR_STATUS='CURRENT'\n",
    "      AND cm.BUSINESS_SEGMENT IN ('270','250')\n",
    "      AND COALESCE(am.ONLINE_ACTUAL_BAL,0) < 0\n",
    "      AND am.CUSTOMER IS NOT NULL\n",
    "),\n",
    "SELECT\n",
    "    s.SNAPSHOT_DATE,\n",
    "    al.CUSTOMER AS CUSTOMER_ID,\n",
    "    case\n",
    "\t    when vas.OVERDUEDAYS >0 then vas.OVERDUEDAYS\n",
    "\t    else 0\n",
    "\t    end as cc_OVERDUEDAYS,\n",
    "    1 AS HAS_CREDIT_CARD\n",
    "FROM SnapshotDates s\n",
    "LEFT JOIN stgke.STG_OVERDUE_VAS vas\n",
    "       ON vas.EXTRACTION_DATE = s.SNAPSHOT_DATE\n",
    "      AND vas.PRODUCT_NAME NOT IN (\n",
    "            'TZ USD PERSONAL credit GOLD',\n",
    "            'TZS Personal Gold credit card',\n",
    "            'Uganda Shillings Personal Gold Credit Card',\n",
    "            'Uganda USD Visa Gold Credit',\n",
    "            'Uganda business Credit',\n",
    "            'TZ USD Business Gold credit',\n",
    "            'TZS Business Gold Credit Card',\n",
    "            'TZ Person Classic credit card',\n",
    "            'Uganda Visa Classic Credit',\n",
    "            'Uganda USD Business Credit',\n",
    "            'TZ USD BUSSINESS credit SILVER'\n",
    "      )\n",
    "LEFT JOIN dbcba.KE_Accounts_List al\n",
    "       ON vas.BANKACCOUNT = al.ACCOUNT_NUMBER\n",
    "WHERE al.CUSTOMER IN (SELECT CUSTOMER FROM FilteredCustomers)\n",
    "ORDER BY s.SNAPSHOT_DATE, al.CUSTOMER;\n",
    "\"\"\")\n",
    "cc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = base_df \\\n",
    "    .merge(dto_df, on=['CUSTOMER_ID','SNAPSHOT_DATE'], how='left') \\\n",
    "    .merge(cto_df, on=['CUSTOMER_ID','SNAPSHOT_DATE'], how='left') \\\n",
    "    .merge(FID_df, on=['CUSTOMER_ID','SNAPSHOT_DATE'], how='left') \\\n",
    "    .merge(maxdpd_df, on=['CUSTOMER_ID','SNAPSHOT_DATE'], how='left') \\\n",
    "    .merge(cc_df, on=['CUSTOMER_ID','SNAPSHOT_DATE'], how='left') \\\n",
    "     .merge(unpaid_df, on=['CUSTOMER_ID','SNAPSHOT_DATE'], how='left') \\\n",
    "    .merge(ppc_df, on=['CUSTOMER_ID','SNAPSHOT_DATE'], how='left') \\\n",
    "    .merge(avg_bal_df, on=['CUSTOMER_ID','SNAPSHOT_DATE'], how='left')\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save final df to Parquet\n",
    "df_final.to_parquet('final_feature_set.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read parquet back to confirm\n",
    "final_check_df = pd.read_parquet('final_feature_set.parquet')\n",
    "final_check_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cursor\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, query in enumerate(tqdm(sql_queries, desc=\"Executing queries\")):\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    df = pd.DataFrame(results, columns=[desc[0] for desc in cursor.description])\n",
    "    dataframes[f\"df_{i+1}\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dataframes.items():\n",
    "    print(f\"DataFrame {key}:\")\n",
    "    print(df.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df for df in dataframes.values()]\n",
    "#the data frames\n",
    "ctos_trended = df_list[0] #CUSTOMER\n",
    "dtos_trended = df_list[1] #CUSTOMER\n",
    "unpaid_items_trended = df_list[2] #CUSTOMER\n",
    "cust_static_data = df_list[3] #CUSTOMER_NUMBER\n",
    "max_dpds = df_list[4] #CUSTOMER\n",
    "loan_to_income_ratio = df_list[5] #CUSTOMER_ID\n",
    "avg_bal_trended = df_list[6] #CUSTOMER\n",
    "fids = df_list[7] #customer_number\n",
    "train_test__crb_raw = df_list[8] #CUSTOMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test__crb_raw['CUSTOMER'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_to_income_ratio.sort_values(by=['TOTAL_LOANS'], ascending=False).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to merge all the dataframes\n",
    "def merge_dataframes(dfs, merge_cols):\n",
    "    \"\"\"\n",
    "    Merges multiple DataFrames on their respective merge columns, ensuring all records from the first df are maintained.\n",
    "    Handles missing values in merge columns by dropping them and storing them separately.\n",
    "    \n",
    "    Parameters:\n",
    "    dfs (list of pd.DataFrame): List of DataFrames to merge.\n",
    "    merge_cols (list of str): List of column names used for merging, corresponding to each df.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (Merged DataFrame, DataFrame of dropped rows with their source DataFrame name)\n",
    "    \"\"\"\n",
    "    if not dfs or len(dfs) != len(merge_cols):\n",
    "        raise ValueError(\"Number of DataFrames and merge column names must be the same and non-empty\")\n",
    "\n",
    "    dropped_rows = []\n",
    "    processed_dfs = []\n",
    "\n",
    "    # Convert all column names to uppercase for consistency\n",
    "    for i in range(len(dfs)):\n",
    "        dfs[i].columns = dfs[i].columns.str.upper()\n",
    "        merge_cols[i] = merge_cols[i].upper()\n",
    "    \n",
    "    # Convert all merge columns to string type and handle missing values\n",
    "    for i, (df, col) in enumerate(zip(dfs, merge_cols)):\n",
    "        if col not in df.columns:\n",
    "            print(f\"DataFrame {i} available columns: {df.columns.tolist()}\")\n",
    "            raise KeyError(f\"Merge column '{col}' not found in DataFrame {i}.\")\n",
    "        \n",
    "        df = df.copy()\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        \n",
    "        # Identify and store dropped rows\n",
    "        missing_rows = df[df[col].isna() | (df[col] == \"\")].copy()\n",
    "        if not missing_rows.empty:\n",
    "            missing_rows['source_df'] = f'DataFrame_{i}'\n",
    "            dropped_rows.append(missing_rows)\n",
    "        \n",
    "        # Drop rows where merge column is missing\n",
    "        df = df.dropna(subset=[col])\n",
    "        df = df[df[col] != \"\"]\n",
    "        processed_dfs.append(df)\n",
    "\n",
    "    # Store dropped rows in a separate DataFrame\n",
    "    dropped_df = pd.concat(dropped_rows, ignore_index=True) if dropped_rows else pd.DataFrame()\n",
    "    \n",
    "    # Use the first DataFrame as the base\n",
    "    merged_df = processed_dfs[0].copy()\n",
    "    base_merge_col = merge_cols[0]\n",
    "    \n",
    "    # Merge each additional DataFrame\n",
    "    for i in range(1, len(processed_dfs)):\n",
    "        df, col = processed_dfs[i], merge_cols[i]\n",
    "        print(f\"Merging with DataFrame {i} on column '{col}'\")  # Debug message\n",
    "        \n",
    "        if col not in df.columns:\n",
    "            print(f\"DataFrame {i} available columns: {df.columns.tolist()}\")\n",
    "            raise KeyError(f\"Merge column '{col}' not found in DataFrame {i}.\")\n",
    "        \n",
    "        merged_df = merged_df.merge(df, left_on=base_merge_col, right_on=col, how='left')\n",
    "        \n",
    "        # Drop the redundant merge column from the joined DataFrame only if it's not the base column\n",
    "        if col != base_merge_col and col in merged_df.columns:\n",
    "            merged_df.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    return merged_df, dropped_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpaid_items_trended.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply\n",
    "dfs = [train_test__crb_raw, ctos_trended, dtos_trended, unpaid_items_trended, cust_static_data, max_dpds, loan_to_income_ratio, avg_bal_trended, fids]\n",
    "merge_cols = ['CUSTOMER', 'CUSTOMER', 'CUSTOMER','CUSTOMER', 'CUSTOMER_NUMBER', 'CUSTOMER', 'CUSTOMER_ID', 'CUSTOMER', 'customer_number']\n",
    "merged_df, dropped_df = merge_dataframes(dfs, merge_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this for columns to drop\n",
    "potential_drop_cols = ['AVG_INCOME', 'TOTAL_LOANS', 'MAX_OD_DAYS', 'MAX_LOAN_DPD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a target column\n",
    "#merged_df['TARGET'] = merged_df['MAX_DPD_TRAIN_TEST'].apply(lambda x: 1 if x >0 and x<=5 else 0)\n",
    "merged_df['TARGET'] = merged_df['MAX_DPD_TRAIN_TEST'].apply(lambda x: 1 if pd.notna(x) and 0 < x <= 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.sort_values(by=['MAX_DPD_TRAIN_TEST'], ascending=False).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column\n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_table(merged_df).head(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missingness visually\n",
    "missing_values_per_variable = missing_values_table(merged_df).rename_axis('Variables').reset_index() \n",
    "\n",
    "# plt.figure(figsize = (18, 8))\n",
    "sns.barplot(data = missing_values_per_variable, x = 'Variables', y = '% of Total Values')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.title(\"Proportion of Missing Values per Variable\")\n",
    "plt.ylabel(\"Percentage of Missing Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in merged_df.columns.values if col not in ['CUSTOMER', 'MAX_DPD_TRAIN_TEST']]\n",
    "\n",
    "merged_df[features].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of the features\n",
    "\n",
    "#merged_df[features].hist(figsize = (15, 30), layout = (10, 3)); \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define number of features\n",
    "num_features = len(features)\n",
    "\n",
    "# Calculate optimal rows and columns\n",
    "cols = 5  # You can adjust this\n",
    "rows = math.ceil(num_features / cols)\n",
    "\n",
    "# Plot histograms\n",
    "merged_df[features].hist(figsize=(15, rows * 3), layout=(rows, cols))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2 = merged_df[['TU_NUMBEROF_ACCOUNTS', 'TU_OPENACCOUNT',\n",
    "       'TU_PERFORMING', 'TU_NONPERFORMING', 'TU_CLOSED', 'TU_CURRENTINARREARS',\n",
    "       'TU_ARREARS0DAYS', 'TU_MAXARREARS', 'TU_MAXARREARSLAST12MONTHS',\n",
    "       'TU_MAXARREARSLAST3MONTHS', 'TU_PRINCIPALAMOUNT', 'TU_SCHEDULEDPAYMENTAMT',\n",
    "       'TU_CREDITAPPLICATIONSDONE', 'TU_CURRENTBALANCEAMOUNT',\n",
    "       'TU_SUBSCRIBERCURRENTBALANCEAMOUNT', 'TU_PASTDUEAMOUNT', 'TU_NUMBEROFENQUIRIES',\n",
    "       'TU_ENQUIRIES30DAYS', 'TU_ENQUIRIES90DAYS', 'CRB_TU_SCORE', 'TU_MOBILE_TOTAL',\n",
    "       'TU_MOBILE_PERFORMING', 'TU_MOBILE_NONPERFORMING', 'TU_MOBILE_PRINCIPALAMOUNT',\n",
    "       'M1M2_CTO_AMT_CHANGE_PCT', 'M2M3_CTO_AMT_CHANGE_PCT',\n",
    "       'M3M4_CTO_AMT_CHANGE_PCT', 'M4M5_CTO_AMT_CHANGE_PCT',\n",
    "       'M5M6_CTO_AMT_CHANGE_PCT', 'M6M7_CTO_AMT_CHANGE_PCT',\n",
    "       'M7M8_CTO_AMT_CHANGE_PCT', 'M8M9_CTO_AMT_CHANGE_PCT',\n",
    "       'M9M10_CTO_AMT_CHANGE_PCT', 'M10M11_CTO_AMT_CHANGE_PCT',\n",
    "       'M11M12_CTO_AMT_CHANGE_PCT', 'Q1Q2_CTO_AMT_CHANGE_PCT',\n",
    "       'Q2Q3_CTO_AMT_CHANGE_PCT', 'Q3Q4_CTO_AMT_CHANGE_PCT',\n",
    "       'Q1Q2_CTO_CNT_CHANGE_PCT', 'Q2Q3_CTO_CNT_CHANGE_PCT',\n",
    "       'Q3Q4_CTO_CNT_CHANGE_PCT', 'M1M2_DTO_AMT_CHANGE_PCT',\n",
    "       'M2M3_DTO_AMT_CHANGE_PCT', 'M3M4_DTO_AMT_CHANGE_PCT',\n",
    "       'M4M5_DTO_AMT_CHANGE_PCT', 'M5M6_DTO_AMT_CHANGE_PCT',\n",
    "       'M6M7_DTO_AMT_CHANGE_PCT', 'M7M8_DTO_AMT_CHANGE_PCT',\n",
    "       'M8M9_DTO_AMT_CHANGE_PCT', 'M9M10_DTO_AMT_CHANGE_PCT',\n",
    "       'M10M11_DTO_AMT_CHANGE_PCT', 'M11M12_DTO_AMT_CHANGE_PCT',\n",
    "       'Q1Q2_DTO_AMT_CHANGE_PCT', 'Q2Q3_DTO_AMT_CHANGE_PCT',\n",
    "       'Q3Q4_DTO_AMT_CHANGE_PCT', 'Q1Q2_DTO_CNT_CHANGE_PCT',\n",
    "       'Q2Q3_DTO_CNT_CHANGE_PCT', 'Q3Q4_DTO_CNT_CHANGE_PCT', 'QI_UNPAID_VALUE',\n",
    "       'Q2_UNPAID_VALUE', 'Q3_UNPAID_VALUE', 'Q4_UNPAID_VALUE',\n",
    "       'Q1_UNPAID_COUNTS', 'Q2_UNPAID_COUNTS', 'Q3_UNPAID_COUNTS',\n",
    "       'Q4_UNPAID_COUNTS', 'Q1Q2_UNPAID_AMT_CHANGE_PCT',\n",
    "       'Q2Q3_UNPAID_AMT_CHANGE_PCT', 'Q3Q4_UNPAID_AMT_CHANGE_PCT',\n",
    "       'Q1Q2_UNPAID_CNT_CHANGE_PCT', 'Q2Q3_UNPAID_CNT_CHANGE_PCT',\n",
    "       'Q3Q4_UNPAID_CNT_CHANGE_PCT',\n",
    "       'CURRMONTHPREVMONTH_UNPAID_AMT_CHANGE_PCT',\n",
    "       'CURRMONTHPREVMONTH_UNPAID_CNT_CHANGE_PCT', 'AGE', 'CUST_TENURE_MONTHS',\n",
    "       'M8M9_BAL_AMT_CHANGE_PCT', 'M9M10_BAL_AMT_CHANGE_PCT',\n",
    "       'M10M11_BAL_AMT_CHANGE_PCT', 'M11M12_BAL_AMT_CHANGE_PCT',\n",
    "       'Q1Q2_BAL_AMT_CHANGE_PCT', 'Q2Q3_BAL_AMT_CHANGE_PCT',\n",
    "       'Q3Q4_BAL_AMT_CHANGE_PCT', 'NO_OF_ACCOUNTS_FID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms scaled- an alternative better view to the above\n",
    "%matplotlib inline\n",
    "#axes = features_2.hist(figsize=(15, 30), layout=(59, 3), bins=20)\n",
    "'''\n",
    "# Apply log scale to y-axis for each subplot\n",
    "for ax in axes.flatten():\n",
    "    ax.set_yscale('log')  \n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "#plt.show()'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurtosis and Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coi = ['TU_NUMBEROF_ACCOUNTS', 'TU_OPENACCOUNT',\n",
    "       'TU_PERFORMING', 'TU_NONPERFORMING', 'TU_CLOSED', 'TU_CURRENTINARREARS',\n",
    "       'TU_ARREARS0DAYS', 'TU_MAXARREARS', 'TU_MAXARREARSLAST12MONTHS',\n",
    "       'TU_MAXARREARSLAST3MONTHS', 'TU_PRINCIPALAMOUNT', 'TU_SCHEDULEDPAYMENTAMT',\n",
    "       'TU_CREDITAPPLICATIONSDONE', 'TU_CURRENTBALANCEAMOUNT',\n",
    "       'TU_SUBSCRIBERCURRENTBALANCEAMOUNT', 'TU_PASTDUEAMOUNT', 'TU_NUMBEROFENQUIRIES',\n",
    "       'TU_ENQUIRIES30DAYS', 'TU_ENQUIRIES90DAYS', 'CRB_TU_SCORE', 'TU_MOBILE_TOTAL',\n",
    "       'TU_MOBILE_PERFORMING', 'TU_MOBILE_NONPERFORMING', 'TU_MOBILE_PRINCIPALAMOUNT',\n",
    "       'M1M2_CTO_AMT_CHANGE_PCT', 'M2M3_CTO_AMT_CHANGE_PCT',\n",
    "       'M3M4_CTO_AMT_CHANGE_PCT', 'M4M5_CTO_AMT_CHANGE_PCT',\n",
    "       'M5M6_CTO_AMT_CHANGE_PCT', 'M6M7_CTO_AMT_CHANGE_PCT',\n",
    "       'M7M8_CTO_AMT_CHANGE_PCT', 'M8M9_CTO_AMT_CHANGE_PCT',\n",
    "       'M9M10_CTO_AMT_CHANGE_PCT', 'M10M11_CTO_AMT_CHANGE_PCT',\n",
    "       'M11M12_CTO_AMT_CHANGE_PCT', 'Q1Q2_CTO_AMT_CHANGE_PCT',\n",
    "       'Q2Q3_CTO_AMT_CHANGE_PCT', 'Q3Q4_CTO_AMT_CHANGE_PCT',\n",
    "       'Q1Q2_CTO_CNT_CHANGE_PCT', 'Q2Q3_CTO_CNT_CHANGE_PCT',\n",
    "       'Q3Q4_CTO_CNT_CHANGE_PCT', 'M1M2_DTO_AMT_CHANGE_PCT',\n",
    "       'M2M3_DTO_AMT_CHANGE_PCT', 'M3M4_DTO_AMT_CHANGE_PCT',\n",
    "       'M4M5_DTO_AMT_CHANGE_PCT', 'M5M6_DTO_AMT_CHANGE_PCT',\n",
    "       'M6M7_DTO_AMT_CHANGE_PCT', 'M7M8_DTO_AMT_CHANGE_PCT',\n",
    "       'M8M9_DTO_AMT_CHANGE_PCT', 'M9M10_DTO_AMT_CHANGE_PCT',\n",
    "       'M10M11_DTO_AMT_CHANGE_PCT', 'M11M12_DTO_AMT_CHANGE_PCT',\n",
    "       'Q1Q2_DTO_AMT_CHANGE_PCT', 'Q2Q3_DTO_AMT_CHANGE_PCT',\n",
    "       'Q3Q4_DTO_AMT_CHANGE_PCT', 'Q1Q2_DTO_CNT_CHANGE_PCT',\n",
    "       'Q2Q3_DTO_CNT_CHANGE_PCT', 'Q3Q4_DTO_CNT_CHANGE_PCT', 'QI_UNPAID_VALUE',\n",
    "       'Q2_UNPAID_VALUE', 'Q3_UNPAID_VALUE', 'Q4_UNPAID_VALUE',\n",
    "       'Q1_UNPAID_COUNTS', 'Q2_UNPAID_COUNTS', 'Q3_UNPAID_COUNTS',\n",
    "       'Q4_UNPAID_COUNTS', 'Q1Q2_UNPAID_AMT_CHANGE_PCT',\n",
    "       'Q2Q3_UNPAID_AMT_CHANGE_PCT', 'Q3Q4_UNPAID_AMT_CHANGE_PCT',\n",
    "       'Q1Q2_UNPAID_CNT_CHANGE_PCT', 'Q2Q3_UNPAID_CNT_CHANGE_PCT',\n",
    "       'Q3Q4_UNPAID_CNT_CHANGE_PCT',\n",
    "       'CURRMONTHPREVMONTH_UNPAID_AMT_CHANGE_PCT',\n",
    "       'CURRMONTHPREVMONTH_UNPAID_CNT_CHANGE_PCT', 'AGE', 'CUST_TENURE_MONTHS',\n",
    "       'M8M9_BAL_AMT_CHANGE_PCT', 'M9M10_BAL_AMT_CHANGE_PCT',\n",
    "       'M10M11_BAL_AMT_CHANGE_PCT', 'M11M12_BAL_AMT_CHANGE_PCT',\n",
    "       'Q1Q2_BAL_AMT_CHANGE_PCT', 'Q2Q3_BAL_AMT_CHANGE_PCT',\n",
    "       'Q3Q4_BAL_AMT_CHANGE_PCT', 'NO_OF_ACCOUNTS_FID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew = {}\n",
    "kurt = {}\n",
    "\n",
    "for i in coi:\n",
    "    merged_df[i] = pd.to_numeric(merged_df[i], errors='coerce')  # Converts to float, sets invalid values to NaN\n",
    "    skew[i] = merged_df[i].skew()\n",
    "    kurt[i] = merged_df[i].kurt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dict to df\n",
    "skewness_df = pd.DataFrame(list(skew.items()), columns=['Variable', 'Skewness'])\n",
    "kurtosis_df = pd.DataFrame(list(kurt.items()), columns=['Variable', 'Kurtosis'])\n",
    "\n",
    "skewness_kurtosis_df = pd.merge(skewness_df, kurtosis_df, on='Variable')\n",
    "skewness_kurtosis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 4))\n",
    "\n",
    "plt.plot(list(skew.keys()), list(skew.values()))\n",
    "plt.xticks(rotation = 90, horizontalalignment = 'left')\n",
    "plt.xlabel(\"Value for Skewness\")\n",
    "plt.title(\"Skewness for potential variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpreting Skewness and Kurtosis:**\n",
    "Skewness and kurtosis are statistical measures that describe the shape of a distribution.\n",
    "\n",
    "#### **1. Skewness Interpretation**\n",
    "- **Skewness = 0**  Symmetric distribution\n",
    "- **Skewness > 0**  Right-skewed (longer tail on the right)\n",
    "- **Skewness < 0**  Left-skewed (longer tail on the left)\n",
    "- **High Skewness (> 2 or < -2)**  Strongly asymmetric distribution\n",
    "\n",
    "#### **2. Kurtosis Interpretation**\n",
    "- **Kurtosis = 3**  Normal distribution (Mesokurtic)\n",
    "- **Kurtosis > 3**  Heavy-tailed distribution (Leptokurtic, extreme outliers)\n",
    "- **Kurtosis < 3**  Light-tailed distribution (Platykurtic, fewer extreme values)\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Observations**\n",
    "#### **1. Extremely High Skewness and Kurtosis**\n",
    "- Many variables (e.g., `M1M2_CTO_AMT_CHANGE_PCT`, `M4M5_CTO_AMT_CHANGE_PCT`, `Q3Q4_DTO_AMT_CHANGE_PCT`) have **very high skewness (> 200)** and **extreme kurtosis (> 100,000)**.\n",
    "- **Implication:** These variables have highly skewed distributions with extreme outliers.\n",
    "\n",
    "#### **2. Negative Skewness**\n",
    "- `Q1Q2_UNPAID_AMT_CHANGE_PCT` (-98.61), `Q3Q4_UNPAID_CNT_CHANGE_PCT` (-110.17), and similar variables are negatively skewed.\n",
    "- **Implication:** The majority of values are on the higher end, with a long tail on the left.\n",
    "\n",
    "#### **3. Customer Variable**\n",
    "- `CUSTOMER`: Skewness (-0.29) & Kurtosis (-0.85) are close to 0.\n",
    "- **Implication:** This variable is roughly symmetric and does not have extreme tails.\n",
    "\n",
    "---\n",
    "\n",
    "### **Implication**\n",
    "1. **Severe Outliers Exist**  \n",
    "   - The high kurtosis values indicate the presence of extreme values.\n",
    "   - Needs to do **outlier treatment** (e.g., winsorization, log transformation).  \n",
    "   - Consider robust methods like decision trees or quantile regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 15))\n",
    "# Getting the Upper Triangle of the co-relation matrix\n",
    "matrix = np.triu(merged_df[coi].corr())\n",
    "\n",
    "# using the upper triangle matrix as mask \n",
    "sns.heatmap(merged_df[coi].corr(), annot=True, mask=matrix, cmap = \"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get closely corr cols into a table\n",
    "\n",
    "def extract_high_correlations(df, coi, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Extracts pairs of columns with correlation above a given threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data\n",
    "    - coi: List of columns to check for correlations\n",
    "    - threshold: Minimum correlation value to consider (default is 0.8)\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with columns: Variable_1, Variable_2, Correlation\n",
    "    \"\"\"\n",
    "    corr_matrix = df[coi].corr()\n",
    "    upper_triangle = np.triu(np.ones(corr_matrix.shape), k=1)  # Mask to exclude duplicates and self-correlation\n",
    "    \n",
    "    correlated_pairs = []\n",
    "    \n",
    "    for i in range(len(coi)):\n",
    "        for j in range(i + 1, len(coi)):  # Only consider upper triangle to avoid duplicates\n",
    "            if abs(corr_matrix.iloc[i, j]) >= threshold:\n",
    "                correlated_pairs.append({\n",
    "                    \"Variable_1\": coi[i],\n",
    "                    \"Variable_2\": coi[j],\n",
    "                    \"Correlation\": corr_matrix.iloc[i, j]\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(correlated_pairs)\n",
    "\n",
    "# Apply\n",
    "correlated_vars = extract_high_correlations(merged_df, coi, threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_vars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_vars.sort_values(by=['Correlation'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For unpaid items, QoQ changes for amounts are highly correlated to that of amounts, so can drop the count changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick cols from above to exclude based on business knowledge\n",
    "related_cols_list = [\n",
    "    'TU_MOBILE_PRINCIPALAMOUNT',\n",
    "    'TU_ARREARS0DAYS',\n",
    "    'TU_MOBILE_PERFORMING',\n",
    "    'TU_NONPERFORMING',\n",
    "    'TU_CLOSED',\n",
    "    'TU_SCHEDULEDPAYMENTAMT',\n",
    "    'TU_MOBILE_PRINCIPALAMOUNT',\n",
    "    \n",
    "    'Q2_UNPAID_VALUE',\n",
    "    'Q3_UNPAID_VALUE',\n",
    "    'Q4_UNPAID_VALUE',\n",
    "    'Q1_UNPAID_COUNTS',\n",
    "    'Q2_UNPAID_COUNTS',\n",
    "    'Q3_UNPAID_COUNTS',\n",
    "    'Q4_UNPAID_COUNTS'\n",
    " \n",
    "    ] #'Q1_UNPAID_VALUE',\n",
    "\n",
    "merged_df_1 = merged_df.drop(columns=related_cols_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send data to csv for prod_pipeline set- before sign_transform\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "path = r'C:/Users/dennis.oseki/OneDrive - NCBA Group PLC/Retail/Early_Warning/prediction_data'\n",
    "merged_df_1.to_csv(path + '/raw_data_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform before model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this to transform while keeping the sign which is important in this case\n",
    "#Transforms the data into a more normal-like distribution while preserving the sign.\n",
    "#Handles both positive and negative values, unlike a standard log transform which fails for negatives.\n",
    "#Reduces the effect of extreme values (outliers).\n",
    "\n",
    "def signed_log_transform(feature):\n",
    "    return np.sign(feature) * np.log1p(abs(feature))\n",
    "\n",
    "'''   \n",
    "df_transformed = df.copy()\n",
    "for col in df.columns:\n",
    "    df_transformed[col] = signed_log_transform(df[col])\n",
    "'''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns\n",
    "feature_cols = [col for col in merged_df.columns if col not in ['CUSTOMER', 'TARGET', 'MAX_DPD_TRAIN_TEST']]\n",
    "\n",
    "# Check for non-numeric columns\n",
    "non_numeric_cols = merged_df[feature_cols].select_dtypes(exclude=['number']).columns\n",
    "print(\"Non-numeric feature columns:\", non_numeric_cols.tolist())\n",
    "\n",
    "# Convert feature columns to numeric, coercing errors (non-convertible values will become NaN)\n",
    "merged_df[feature_cols] = merged_df[feature_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Apply the transformation only to feature columns\n",
    "merged_df[feature_cols] = merged_df[feature_cols].applymap(signed_log_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store\n",
    "customer_ids = merged_df_1['CUSTOMER']\n",
    "\n",
    "# Define features and target\n",
    "X = merged_df_1.drop(columns=['CUSTOMER', 'TARGET', 'MAX_DPD_TRAIN_TEST', 'MAX_OD_DAYS', 'MAX_LOAN_DPD'])\n",
    "y = merged_df_1['TARGET']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=37)\n",
    "\n",
    "# Split customer IDs so they match X_test\n",
    "customer_ids_test = customer_ids.iloc[X_test.index]  # Get matching IDs for test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Train RandomForest as the base model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "base_model = RandomForestClassifier()\n",
    "base_model.fit(X_train, y_train)\n",
    "base_preds = base_model.predict_proba(X_train)[:,1]  # Get probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_preds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Train XGBoost model as the first improvement from base\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train, sample_weight=abs(y_train - base_preds))  # Weight by errors\n",
    "xgb_preds = xgb_model.predict_proba(X_train)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_preds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Train final meta_learner model (LightGBM)\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# Stack predictions as new features\n",
    "X_meta = np.column_stack((base_preds, xgb_preds))\n",
    "\n",
    "# Train LightGBM on the stacked predictions\n",
    "lgb_model = lgb.LGBMClassifier()\n",
    "lgb_model.fit(X_meta, y_train)\n",
    "\n",
    "\n",
    "import joblib\n",
    "# Save the model to a file\n",
    "model_path = r'C:/Users/dennis.oseki/OneDrive - NCBA Group PLC/Retail/Early_Warning/saved_model_versions'\n",
    "date_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "#joblib.dump(lgb_model, f\"{model_path}/base_model_{date_time}.pkl\")\n",
    "#joblib.dump(lgb_model, \"meta_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store xgb model\n",
    "#joblib.dump(xgb_model, f\"{model_path}/xgb_model_{date_time}.pkl\")\n",
    "#joblib.dump(lgb_model, \"meta_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Generate final test set predictions using the ensemble\n",
    "base_test_preds = base_model.predict_proba(X_test)[:,1]\n",
    "xgb_test_preds = xgb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "X_test_meta = np.column_stack((base_test_preds, xgb_test_preds))\n",
    "final_preds = lgb_model.predict_proba(X_test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_preds[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, log_loss, confusion_matrix,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_proba, model_name=\"Model Evaluation\"):\n",
    "    \"\"\"\n",
    "    Evaluates a classification model using key metrics and visualizations.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Actual labels\n",
    "    - y_pred: Predicted labels\n",
    "    - y_proba: Predicted probabilities for class 1\n",
    "    - model_name: Title for the visualizations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute key metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_proba)\n",
    "    logloss = log_loss(y_true, y_proba)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n {model_name} Metrics\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"Log Loss: {logloss:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Confusion Matrix Plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{model_name} - Confusion Matrix\")\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\", color=\"blue\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{model_name} - ROC Curve\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_proba)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(recall_vals, precision_vals, label=\"Precision-Recall Curve\", color=\"red\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"{model_name} - Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply \n",
    "# Get predictions from base model\n",
    "y_pred = base_model.predict(X_test)  \n",
    "y_proba = base_model.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(y_test, y_pred, y_proba, model_name=\"RF model Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply \n",
    "# Get predictions XGBoost- second model\n",
    "y_pred = xgb_model.predict(X_test)  \n",
    "y_proba = xgb_model.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(y_test, y_pred, y_proba, model_name=\"XGBoost Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply to final model\n",
    "# Get predictions from your trained XGBoost model\n",
    "y_pred = lgb_model.predict(X_test_meta)  \n",
    "y_proba = lgb_model.predict_proba(X_test_meta)[:, 1]  # Probability of class 1\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(y_test, y_pred, y_proba, model_name=\"Ensembled model Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining back to original data to get customer numbers to predictions of the ensembled model\n",
    "\n",
    "test_set_predictions = pd.DataFrame({\n",
    "    'CUSTOMER': customer_ids_test,  # Restore customer IDs\n",
    "    'TARGET': y_test.values,  # Convert to avoid index mismatches\n",
    "    'PREDICTIONS': y_pred,\n",
    "    'PROBABILITIES': y_proba\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set_predictions.sort_values(by=['PROBABILITIES'], ascending=False).tail()\n",
    "test_set_predictions.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create global variables for reuse\n",
    "results_path = r'C:/Users/dennis.oseki/OneDrive - NCBA Group PLC/Retail/Early_Warning/Test_Results'\n",
    "date_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send results to xlsx\n",
    "test_set_predictions.to_excel(f\"{results_path}/test_set_predictions_{date_time}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine back to original df to get all columns\n",
    "X_test_original = merged_df.loc[X_test.index]  # Get original X_test rows with all columns\n",
    "\n",
    "test_set_predictions_all = X_test_original.merge(test_set_predictions, on='CUSTOMER', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_predictions_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_predictions_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature importance\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "\n",
    "# pair with the corresponding feature names for better interpretation\n",
    "feature_names = X.columns \n",
    "feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
    "sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('Feature importance V1.0:')\n",
    "print()\n",
    "#Print\n",
    "for feature, importance in sorted_feature_importance:\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send feature importance to xlsx\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": feature_importances.astype(float)\n",
    "}).round(4)\n",
    "\n",
    "# Sort by importance\n",
    "feature_importance_sorted = feature_importance_df.sort_values(by=\"Importance\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send feature importance to xlsx\n",
    "#feature_importance_sorted.to_excel(f\"{results_path}/feature_importance_{date_time}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative upstacking approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "#from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models with hyperparameters\n",
    "''' \n",
    "base_models = [\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=207, random_state=27)),\n",
    "    (\"xgb\", XGBClassifier(n_estimators=207, learning_rate=0.15, random_state=27, use_label_encoder=False)),\n",
    "    (\"lgbm\", LGBMClassifier(n_estimators=207, learning_rate=0.15, random_state=27)),\n",
    "    (\"cat\", CatBoostClassifier(iterations=207, learning_rate=0.15, random_state=27, verbose=0))  # Set verbose=0 to suppress logs\n",
    "]\n",
    "'''\n",
    "\n",
    "#Using GridSearch to tune hyperparams including class weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights based on training labels\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train)\n",
    "class_weight_dict = {cls: weight for cls, weight in zip(classes, class_weights)}\n",
    "\n",
    "# Define base models with class weights\n",
    "base_models = {\n",
    "    \"rf\": RandomForestClassifier(n_estimators=207, random_state=27, class_weight=\"balanced\"),\n",
    "    \"xgb\": XGBClassifier(n_estimators=207, learning_rate=0.15, random_state=27, use_label_encoder=False, \n",
    "                         scale_pos_weight=class_weight_dict[1]),  # Weighting the positive class\n",
    "    \"lgbm\": LGBMClassifier(n_estimators=207, learning_rate=0.15, random_state=27, class_weight=\"balanced\"),\n",
    "   # \"cat\": CatBoostClassifier(iterations=207, learning_rate=0.15, random_state=27, verbose=0, scale_pos_weight=class_weight_dict[1])\n",
    "}\n",
    "\n",
    "\n",
    "# Define hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    \"rf\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"max_depth\": [None, 10, 20, 30],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "    \"xgb\": {\n",
    "        \"learning_rate\": [0.05, 0.1, 0.15],\n",
    "        \"max_depth\": [3, 6, 9],\n",
    "        \"subsample\": [0.7, 0.8, 1.0]\n",
    "    },\n",
    "    \"lgbm\": {\n",
    "        \"learning_rate\": [0.05, 0.1, 0.15],\n",
    "        \"max_depth\": [-1, 5, 10],\n",
    "        \"num_leaves\": [31, 50, 100]\n",
    "    },\n",
    "    \"cat\": {\n",
    "        \"learning_rate\": [0.05, 0.1, 0.15],\n",
    "        \"depth\": [4, 6, 8],\n",
    "        \"l2_leaf_reg\": [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define scoring metric (optimize for recall)\n",
    "scoring_metric = make_scorer(recall_score)\n",
    "\n",
    "# Perform Grid Search for each model\n",
    "best_params = {}\n",
    "\n",
    "for model_name, model in base_models.items():\n",
    "    print(f\"Running GridSearchCV for {model_name}...\")\n",
    "    grid_search = GridSearchCV(model, param_grids[model_name], cv=5, scoring=scoring_metric, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)  # Make sure X_train and y_train are defined\n",
    "    \n",
    "    best_params[model_name] = grid_search.best_params_\n",
    "    print(f\"Best params for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best Recall Score: {grid_search.best_score_}\\n\")\n",
    "\n",
    "# Print all best parameters\n",
    "print(\"Final Best Hyperparameters for Each Model:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array to store meta-features\n",
    "'''\n",
    "meta_features = np.zeros((X_train.shape[0], len(base_models)))\n",
    "\n",
    "# Generate OOF predictions\n",
    "for i, (name, model) in enumerate(base_models):\n",
    "    meta_features[:, i] = cross_val_predict(\n",
    "        model, X_train, y_train, cv=5, method=\"predict_proba\"\n",
    "    )[:, 1]  # Take probability of the positive class\n",
    "'''\n",
    "\n",
    "'''\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty array to store meta-features\n",
    "meta_features = np.zeros((X_train.shape[0], len(base_models)))\n",
    "\n",
    "# Use best hyperparameters from GridSearchCV results\n",
    "optimized_models = {\n",
    "    \"rf\": RandomForestClassifier(**best_params[\"rf\"], random_state=27, class_weight=\"balanced\"),\n",
    "    \"xgb\": XGBClassifier(**best_params[\"xgb\"], random_state=27, use_label_encoder=False,\n",
    "                         scale_pos_weight=class_weight_dict[1]),\n",
    "    \"lgbm\": LGBMClassifier(**best_params[\"lgbm\"], random_state=27, class_weight=\"balanced\"),\n",
    "    #\"cat\": CatBoostClassifier(**best_params[\"cat\"], random_state=27, verbose=0, scale_pos_weight=class_weight_dict[1])\n",
    "}\n",
    "\n",
    "# Generate Out-of-Fold (OOF) predictions\n",
    "for i, (name, model) in enumerate(optimized_models.items()):\n",
    "    print(f\"Generating OOF predictions for {name}...\")\n",
    "    meta_features[:, i] = cross_val_predict(\n",
    "        model, X_train, y_train, cv=5, method=\"predict_proba\"\n",
    "    )[:, 1]  # Take probability of the positive class\n",
    "\n",
    "# Train meta-model on OOF predictions\n",
    "meta_model = XGBClassifier(random_state=27) \n",
    "meta_model.fit(meta_features, y_train)\n",
    "\n",
    "# Generate final predictions on test data\n",
    "test_meta_features = np.zeros((X_test.shape[0], len(optimized_models)))\n",
    "for i, (name, model) in enumerate(optimized_models.items()):\n",
    "    model.fit(X_train, y_train)  # Fit each base model\n",
    "    test_meta_features[:, i] = model.predict_proba(X_test)[:, 1]  # Take probability of positive class\n",
    "\n",
    "# Final prediction using meta-model\n",
    "final_preds = meta_model.predict(test_meta_features)\n",
    "\n",
    "# Print performance metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, final_preds))'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Use best hyperparameters from GridSearchCV results\n",
    "optimized_models = {\n",
    "    \"rf\": RandomForestClassifier(**best_params[\"rf\"], random_state=27, class_weight=\"balanced\"),\n",
    "    \"xgb\": XGBClassifier(**best_params[\"xgb\"], random_state=27, use_label_encoder=False,\n",
    "                         scale_pos_weight=class_weight_dict[1]),\n",
    "    \"lgbm\": LGBMClassifier(**best_params[\"lgbm\"], random_state=27, class_weight=\"balanced\"),\n",
    "    #\"cat\": CatBoostClassifier(**best_params[\"cat\"], random_state=27, verbose=0, scale_pos_weight=class_weight_dict[1])\n",
    "}\n",
    "\n",
    "# Create an empty array to store OOF predictions\n",
    "meta_features = np.zeros((X_train.shape[0], len(optimized_models)))\n",
    "\n",
    "# Generate Out-of-Fold (OOF) predictions\n",
    "for i, (name, model) in enumerate(optimized_models.items()):\n",
    "    print(f\"Generating OOF predictions for {name}...\")\n",
    "    meta_features[:, i] = cross_val_predict(\n",
    "        model, X_train, y_train, cv=5, method=\"predict_proba\"\n",
    "    )[:, 1]  # Take probability of the positive class\n",
    "\n",
    "# Concatenate original features + base model OOF predictions\n",
    "meta_train = np.hstack([X_train, meta_features])\n",
    "\n",
    "# Train meta-model\n",
    "meta_model = XGBClassifier(random_state=27) \n",
    "meta_model.fit(meta_train, y_train)\n",
    "\n",
    "# Generate test set base model predictions\n",
    "test_meta_features = np.zeros((X_test.shape[0], len(optimized_models)))\n",
    "for i, (name, model) in enumerate(optimized_models.items()):\n",
    "    model.fit(X_train, y_train)  # Fit each base model\n",
    "    test_meta_features[:, i] = model.predict_proba(X_test)[:, 1]  # Take probability of positive class\n",
    "\n",
    "# Concatenate original test features + base model test predictions\n",
    "meta_test = np.hstack([X_test, test_meta_features])\n",
    "\n",
    "# Final prediction using meta-model\n",
    "final_preds = meta_model.predict(meta_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(classification_report(y_test, final_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta_model = XGBClassifier(n_estimators=217, learning_rate=0.05, random_state=42,  scale_pos_weight=class_weight_dict[1])\n",
    "#meta_model.fit(meta_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test fine tuning further\n",
    "'''\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "# Get predicted probabilities for the positive class\n",
    "probs = meta_model.predict_proba(test_meta_features)[:, 1]\n",
    "\n",
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, probs)\n",
    "\n",
    "# Choose a threshold that maximizes recall while keeping decent precision\n",
    "desired_recall = 0.70  # Adjust based on trade-off preference\n",
    "best_threshold_idx = np.argmax(recall >= desired_recall)\n",
    "best_threshold = thresholds[best_threshold_idx]\n",
    "\n",
    "# Apply new threshold\n",
    "final_preds = (probs >= best_threshold).astype(int)\n",
    "# Generate test set predictions for base models using optimized hyperparameters\n",
    "meta_test = np.column_stack([\n",
    "    optimized_models[name].fit(X_train, y_train).predict_proba(X_test)[:, 1] \n",
    "    for name in optimized_models\n",
    "])\n",
    "\n",
    "# Predict final outcome using meta-learner\n",
    "final_predictions = meta_model.predict(meta_test)'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(optimized_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply to alternate final\n",
    "# Get preds\n",
    "#y_pred = lgb_model.predict(X_test_meta)  \n",
    "#y_proba = lgb_model.predict_proba(X_test_meta)[:, 1]  # Probability of class 1\n",
    "\n",
    "# Predict final outcome using meta-learner\n",
    "y_pred = meta_model.predict(meta_test)           # Binary predictions\n",
    "y_proba = meta_model.predict_proba(meta_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(y_test, y_pred, y_proba, model_name=\"Alternate Ensemble Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find best threshold for high recall\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "# Find the threshold where recall is at least 80% (or as high as possible)\n",
    "recall_target = 0.80  # Adjust this if needed\n",
    "best_idx = (recall >= recall_target).nonzero()[0][-1]  # Last index where recall is still >= 80%\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(f\"Best Threshold for High Recall: {best_threshold:.4f}\")\n",
    "\n",
    "# Apply the threshold\n",
    "#input_data_2[\"predicted_class\"] = (y_proba >= best_threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early risk detection- should favor recall over precision (catch more defaulters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "model_path = r'C:/Users/dennis.oseki/OneDrive - NCBA Group PLC/Retail/Early_Warning/saved_model_versions'\n",
    "date_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define a directory to store models\n",
    "model_path = r'C:/Users/dennis.oseki/OneDrive - NCBA Group PLC/Retail/Early_Warning/saved_model_versions'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# Timestamp for versioning\n",
    "date_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# Save all base models\n",
    "for name, model in optimized_models.items():\n",
    "    joblib.dump(model, f\"{model_path}/base_model_{name}_{date_time}.pkl\")\n",
    "\n",
    "# Save the meta-model\n",
    "joblib.dump(meta_model, f\"{model_path}/meta_model_{date_time}.pkl\")\n",
    "\n",
    "print(f\"Models saved in {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the transformer\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Custom Transformer Class\n",
    "class SignedLogTransformer:\n",
    "    def transform(self, feature):\n",
    "        return np.sign(feature) * np.log1p(abs(feature))\n",
    "\n",
    "# Instantiate transformer\n",
    "log_transformer = SignedLogTransformer()\n",
    "\n",
    "# Save the transformer\n",
    "joblib.dump(log_transformer, f\"{model_path}/log_transformer_{date_time}.pkl\")\n",
    "print(\"Transformer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model prediction feature names\n",
    "base_model_features = [\"rf_pred\", \"xgb_pred\", \"lgbm_pred\"]\n",
    "\n",
    "#Feature importance\n",
    "feature_importances_2 = meta_model.feature_importances_\n",
    "\n",
    "# Combine original feature names with base model predictions\n",
    "aligned_feature_names = list(X.columns) + base_model_features\n",
    "\n",
    "# Check if the lengths now match\n",
    "if len(feature_importances_2) == len(aligned_feature_names):\n",
    "    # Create DataFrame\n",
    "    feature_importance_df_meta = pd.DataFrame({\n",
    "        \"Feature\": aligned_feature_names,\n",
    "        \"Importance\": feature_importances_2.astype(float)\n",
    "    }).round(6)\n",
    "\n",
    "    # Sort by importance\n",
    "    feature_importance_sorted_meta = feature_importance_df_meta.sort_values(\n",
    "        by=\"Importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Save to Excel\n",
    "    #feature_importance_sorted_meta.to_excel(\"feature_importance_meta.xlsx\", index=False)\n",
    "    #print(\"Feature importance saved successfully!\")\n",
    "else:\n",
    "    print(f\"Length mismatch: {len(feature_importances_2)} importances vs {len(aligned_feature_names)} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_sorted_meta.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_importances_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send feature importance to xlsx for the meta_model\n",
    "feature_importance_sorted_meta.to_excel(f\"{results_path}/feature_importance_meta_{date_time}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison: Alternate Ensemble vs. XGBoost\n",
    "\n",
    "#### **1. Accuracy**\n",
    "- **Ensemble:** 71.27%\n",
    "- **XGBoost:** 69.02%\n",
    "- **Better:** **Ensemble**\n",
    "\n",
    "#### **2. Precision**\n",
    "- **Ensemble:** 65.81%\n",
    "- **XGBoost:** 61.54\n",
    "- **Better:** **Ensemble** (fewer false positives)\n",
    "\n",
    "#### **3. Recall**\n",
    "- **Ensemble:** 57.8%\n",
    "- **XGBoost:** 62.03%\n",
    "- **Better:** **XGBoost** (fewer false negatives, better at catching positives)\n",
    "\n",
    "#### **4. F1-score**\n",
    "- **Ensemble:** 61.54%\n",
    "- **XGBoost:** 61.78%\n",
    "- **Better:** **XGBoost** (better balance between precision & recall)\n",
    "\n",
    "#### **5. ROC AUC (Discrimination Power)**\n",
    "- **Ensemble:** 0.7811\n",
    "- **XGBoost:** 0.7668\n",
    "- **Better:** **Ensemble** (better at ranking positive instances higher than negative ones)\n",
    "\n",
    "#### **6. Log Loss (Lower is better)**\n",
    "- **Ensemble:** 0.5329\n",
    "- **XGBoost:** 0.5454\n",
    "- **Better:** **Ensemble** (better probability calibration)\n",
    "\n",
    "### **Overall Conclusion**\n",
    "- **If you prioritize recall (catching more positive cases, reducing false negatives)**  **XGBoost is better.**\n",
    "- **If you prioritize precision (reducing false positives) and overall ranking quality (AUC, log loss)**  **Ensemble is better.**\n",
    "- **For a balanced trade-off, the ensemble appears to be slightly superior.** It has better precision, ROC AUC, log loss, and accuracy, although XGBoost has better recall and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncbawork-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
